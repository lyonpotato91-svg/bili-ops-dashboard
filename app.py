import re
import time
import io
import sqlite3
import requests
import numpy as np
import pandas as pd
import streamlit as st
import plotly.express as px

st.set_page_config(page_title="Bç«™è¿è¥æ•°æ®Dashboard", layout="wide")

# =========================
# Constants
# =========================
BASELINE_PROJECT = "__BASELINE__"       # éšè—é¡¹ç›®ï¼šä¸å‡ºç°åœ¨é¡¹ç›®å½’æ¡£/ç­›é€‰é‡Œ
DB_PATH = "bili_dashboard.db"           # SQLiteæ–‡ä»¶ï¼ˆæŒä¹…åŒ–ï¼‰
TABLE_NAME = "videos"
HEADERS = {"User-Agent": "Mozilla/5.0"}

# =========================
# DB
# =========================
def db_conn():
    return sqlite3.connect(DB_PATH, check_same_thread=False)

def init_db():
    with db_conn() as conn:
        conn.execute(f"""
        CREATE TABLE IF NOT EXISTS {TABLE_NAME} (
            project TEXT NOT NULL,
            bvid TEXT NOT NULL,
            url TEXT,
            title TEXT,
            pubdate TEXT,
            owner_mid TEXT,
            owner_name TEXT,
            view INTEGER,
            like INTEGER,
            coin INTEGER,
            favorite INTEGER,
            reply INTEGER,
            danmaku INTEGER,
            share INTEGER,
            fans_delta INTEGER,
            baseline_for TEXT,
            data_type TEXT,
            fetched_at TEXT,
            PRIMARY KEY (project, bvid)
        )
        """)
        conn.commit()

def load_all_rows() -> pd.DataFrame:
    init_db()
    with db_conn() as conn:
        return pd.read_sql_query(f"SELECT * FROM {TABLE_NAME}", conn)

def upsert_rows(df_new: pd.DataFrame):
    if df_new is None or df_new.empty:
        return
    init_db()
    cols = [
        "project","bvid","url","title","pubdate","owner_mid","owner_name",
        "view","like","coin","favorite","reply","danmaku","share","fans_delta",
        "baseline_for","data_type","fetched_at"
    ]
    df_new = df_new.copy()
    for c in cols:
        if c not in df_new.columns:
            df_new[c] = None
    df_new = df_new[cols]

    records = []
    for _, r in df_new.iterrows():
        records.append(tuple(None if pd.isna(v) else v for v in r.tolist()))

    placeholders = ",".join(["?"] * len(cols))
    colnames = ",".join(cols)
    sql = f"INSERT OR REPLACE INTO {TABLE_NAME} ({colnames}) VALUES ({placeholders})"
    with db_conn() as conn:
        conn.executemany(sql, records)
        conn.commit()

def clear_all_data():
    init_db()
    with db_conn() as conn:
        conn.execute(f"DELETE FROM {TABLE_NAME}")
        conn.commit()

# =========================
# Utils
# =========================
NUM_COLS = ["view", "like", "coin", "favorite", "reply", "danmaku", "share", "fans_delta"]
EXTRA_COLS = ["baseline_for", "data_type"]

def parse_bvid(url_or_bv: str) -> str | None:
    s = (url_or_bv or "").strip()
    m = re.search(r"(BV[0-9A-Za-z]{10})", s)
    return m.group(1) if m else None

def _safe_int(x, default=0):
    try:
        if pd.isna(x):
            return default
        if isinstance(x, str):
            x = x.replace(",", "").strip()
        return int(float(x))
    except Exception:
        return default

def _safe_str(x, default=""):
    try:
        if pd.isna(x):
            return default
        return str(x)
    except Exception:
        return default

def _safe_date(x):
    try:
        if pd.isna(x):
            return pd.NaT
        return pd.to_datetime(x, errors="coerce")
    except Exception:
        return pd.NaT

def _norm_mid(x) -> str:
    """mid ç»Ÿä¸€ä¸ºçº¯æ•°å­—å­—ç¬¦ä¸²ï¼›è¶…é•¿midè§†ä¸ºå¼‚å¸¸ã€‚"""
    if x is None or pd.isna(x):
        return ""
    s = str(x).strip()
    if s.endswith(".0"):
        s = s[:-2]
    s = re.sub(r"[^\d]", "", s)
    if not s:
        return ""
    if len(s) > 12:
        return ""
    return s

def normalize_df(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]

    zh_alias = {
        "é¡¹ç›®": "project",
        "é¡¹ç›®å": "project",
        "è§†é¢‘é“¾æ¥": "url",
        "é“¾æ¥": "url",
        "æ ‡é¢˜": "title",
        "UPä¸»": "owner_name",
        "UPä¸»åç§°": "owner_name",
        "å‘å¸ƒæ—¶é—´": "pubdate",
        "æ’­æ”¾": "view",
        "æ’­æ”¾é‡": "view",
        "ç‚¹èµ": "like",
        "æŠ•å¸": "coin",
        "æ”¶è—": "favorite",
        "è¯„è®º": "reply",
        "å¼¹å¹•": "danmaku",
        "åˆ†äº«": "share",
        "ç²‰ä¸å¢é•¿": "fans_delta",
        "ç²‰ä¸å¢é‡": "fans_delta",
        "BV": "bvid",
        "BVå·": "bvid",
        "bvid": "bvid",
        "owner_mid": "owner_mid",
        "mid": "owner_mid",
        "åŸºå‡†å½’å±": "baseline_for",
        "æ•°æ®ç±»å‹": "data_type",
        "æŠ“å–æ—¶é—´": "fetched_at",
    }

    rename = {}
    for c in df.columns:
        key = str(c).strip()
        if key in zh_alias:
            rename[c] = zh_alias[key]
        else:
            low = key.lower()
            if low in [
                "project","url","bvid","title","owner_name","owner_mid","pubdate",
                "view","like","coin","favorite","reply","danmaku","share","fans_delta",
                "baseline_for","data_type","fetched_at"
            ]:
                rename[c] = low
    df = df.rename(columns=rename)

    if "bvid" not in df.columns and "url" in df.columns:
        df["bvid"] = df["url"].apply(parse_bvid)

    if "bvid" in df.columns:
        df["bvid"] = df["bvid"].apply(lambda x: parse_bvid(x) if isinstance(x, str) else x)
        df["bvid"] = df["bvid"].apply(lambda x: _safe_str(x))

    for col in ["project", "title", "owner_name"]:
        if col not in df.columns:
            df[col] = ""
    for col in EXTRA_COLS:
        if col not in df.columns:
            df[col] = ""

    if "owner_mid" not in df.columns:
        df["owner_mid"] = ""
    df["owner_mid"] = df["owner_mid"].apply(_norm_mid)

    if "pubdate" not in df.columns:
        df["pubdate"] = pd.NaT
    df["pubdate"] = df["pubdate"].apply(_safe_date)

    for col in NUM_COLS:
        if col not in df.columns:
            df[col] = 0
        df[col] = df[col].apply(_safe_int)

    if "fetched_at" not in df.columns:
        df["fetched_at"] = pd.Timestamp.now()
    df["fetched_at"] = pd.to_datetime(df["fetched_at"], errors="coerce").fillna(pd.Timestamp.now())

    keep = set([
        "project","bvid","url","title","pubdate","owner_mid","owner_name",
        "view","like","coin","favorite","reply","danmaku","share","fans_delta",
        "baseline_for","data_type","fetched_at"
    ])
    df = df[[c for c in df.columns if c in keep]].copy()
    if "bvid" in df.columns:
        df = df[df["bvid"].astype(str).str.startswith("BV")]
    return df

def compute_metrics(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df["engagement"] = df["like"] + df["coin"] + df["favorite"] + df["reply"]
    df["engagement_rate"] = np.where(df["view"] > 0, df["engagement"] / df["view"], 0.0)
    df["deep_signal_ratio"] = np.where(
        df["engagement"] > 0, (df["coin"] + df["favorite"]) / df["engagement"], 0.0
    )
    return df

def _sort_owner_hist(df_owner: pd.DataFrame) -> pd.DataFrame:
    g = df_owner.copy()
    g["__sort_time"] = g["pubdate"]
    missing = g["__sort_time"].isna()
    g.loc[missing, "__sort_time"] = g.loc[missing, "fetched_at"]
    g = g[pd.notna(g["__sort_time"])].sort_values("__sort_time", ascending=False)
    return g

# =========================
# Performance labels (ç”¨äºTop/Bottomå’Œè¡¨æ ¼çš„â€œå‘æŒ¥â€æ ‡ç­¾)
# =========================
def perf_label(value: float, baseline_values: np.ndarray, ratio_hi: float, ratio_lo: float, min_n: int) -> str:
    baseline_values = baseline_values[~np.isnan(baseline_values)]
    if len(baseline_values) < min_n:
        return "åŸºå‡†ä¸è¶³"
    med = float(np.median(baseline_values))
    ratio = (value / med) if med > 1e-12 else np.inf
    if ratio >= ratio_hi:
        return "è¶…å¸¸å‘æŒ¥"
    if ratio <= ratio_lo:
        return "ä½äºé¢„æœŸ"
    return "æ­£å¸¸å‘æŒ¥"

def build_owner_cache(df_all: pd.DataFrame) -> dict:
    cache = {}
    for up, g in df_all.groupby("owner_name"):
        cache[up] = _sort_owner_hist(g)
    return cache

def recent_baseline(owner_hist_desc: pd.DataFrame, current_bvid: str, col: str, window_n: int) -> np.ndarray:
    if owner_hist_desc is None or owner_hist_desc.empty:
        return np.array([], dtype=float)
    h = owner_hist_desc[owner_hist_desc["bvid"] != current_bvid]
    if h.empty:
        return np.array([], dtype=float)
    return h.head(window_n)[col].astype(float).to_numpy()

def add_perf_cols(df_show: pd.DataFrame, df_all: pd.DataFrame, window_n: int, min_n: int) -> pd.DataFrame:
    df_show = df_show.copy()
    cache = build_owner_cache(df_all)
    v_labels, er_labels = [], []
    for _, r in df_show.iterrows():
        up = r.get("owner_name", "")
        bvid = r.get("bvid", "")
        owner_hist = cache.get(up, None)
        v_base = recent_baseline(owner_hist, bvid, "view", window_n)
        er_base = recent_baseline(owner_hist, bvid, "engagement_rate", window_n)
        v_labels.append(perf_label(float(r.get("view", 0)), v_base, ratio_hi=1.5, ratio_lo=0.7, min_n=min_n))
        er_labels.append(perf_label(float(r.get("engagement_rate", 0.0)), er_base, ratio_hi=1.3, ratio_lo=0.75, min_n=min_n))
    df_show["æ’­æ”¾è¡¨ç°"] = v_labels
    df_show["äº’åŠ¨ç‡è¡¨ç°"] = er_labels
    return df_show

# =========================
# Bç«™æŠ“å–
# =========================
def fetch_video_detail_by_bvid(bvid: str) -> dict | None:
    api = "https://api.bilibili.com/x/web-interface/view"
    for _ in range(3):
        try:
            r = requests.get(api, params={"bvid": bvid}, headers=HEADERS, timeout=10)
            j = r.json()
            if j.get("code") != 0:
                time.sleep(0.6)
                continue
            d = j["data"]
            stat = d.get("stat", {})
            owner = d.get("owner", {})
            return {
                "bvid": bvid,
                "title": d.get("title"),
                "pubdate": pd.to_datetime(d.get("pubdate", 0), unit="s", errors="coerce"),
                "owner_mid": owner.get("mid"),
                "owner_name": owner.get("name"),
                "view": stat.get("view", 0),
                "like": stat.get("like", 0),
                "coin": stat.get("coin", 0),
                "favorite": stat.get("favorite", 0),
                "reply": stat.get("reply", 0),
                "danmaku": stat.get("danmaku", 0),
                "share": stat.get("share", 0),
            }
        except Exception:
            time.sleep(0.6)
    return None

def fetch_vlist_by_mid(mid: int, n: int = 30) -> list[dict]:
    api = "https://api.bilibili.com/x/space/arc/search"
    out = []
    ps = 50
    pn = 1
    while len(out) < n and pn <= 5:
        r = requests.get(api, params={"mid": mid, "pn": pn, "ps": ps, "order": "pubdate"}, headers=HEADERS, timeout=10)
        j = r.json()
        if j.get("code") != 0:
            break
        vlist = (((j.get("data") or {}).get("list") or {}).get("vlist")) or []
        if not vlist:
            break
        for v in vlist:
            bvid = v.get("bvid")
            if not bvid:
                continue
            out.append({
                "bvid": bvid,
                "title": v.get("title", ""),
                "pubdate": pd.to_datetime(v.get("created", 0), unit="s", errors="coerce"),
                "view": _safe_int(v.get("play", 0)),
                "reply": _safe_int(v.get("comment", 0)),
            })
            if len(out) >= n:
                break
        pn += 1
    return out[:n]

# =========================
# KOL æ ‡æ³¨
# =========================
def kol_flag(view_lift: float | None, er_lift: float | None, deep_lift: float | None) -> str:
    def _v(x):
        try:
            if x is None or (isinstance(x, float) and np.isnan(x)):
                return None
            return float(x)
        except Exception:
            return None
    v, e, d = _v(view_lift), _v(er_lift), _v(deep_lift)
    if (v is not None and v >= 0.30) or (e is not None and e >= 0.20) or (d is not None and d >= 0.10):
        return "â­ åˆä½œæ˜æ˜¾æ›´å¥½"
    if (v is not None and v <= -0.20) or (e is not None and e <= -0.15):
        return "âš ï¸ åˆä½œåå¼±"
    return ""

# =========================
# Sidebar - global settings
# =========================
st.sidebar.title("ğŸ“Š Bç«™è¿è¥Dashboard")

st.sidebar.markdown("#### å…¨å±€â€œå‘æŒ¥è¯„ä»·â€å£å¾„ï¼ˆæŒ‰KOLè‡ªèº«å†å²ï¼Œä¸æŒ‰æ—¶é—´ï¼‰")
baseline_window_n = st.sidebar.slider("åŸºå‡†ï¼šå–è¯¥KOLæœ€è¿‘Næ¡è§†é¢‘ï¼ˆæŒ‰å‘å¸ƒæ—¶é—´/æŠ“å–æ—¶é—´æ’åºï¼‰", 10, 60, 20, step=5)
baseline_min_n = st.sidebar.slider("æœ€ä½æ ·æœ¬æ•°ï¼ˆåªä¸åº“å†…æ¡æ•°æœ‰å…³ï¼‰", 1, 20, 6, step=1)

st.sidebar.divider()

with st.sidebar.expander("å¤‡ä»½/æ¢å¤", expanded=False):
    df_export = load_all_rows()
    if not df_export.empty:
        st.download_button(
            "â¬‡ï¸ å¯¼å‡ºå¤‡ä»½CSV",
            data=df_export.to_csv(index=False).encode("utf-8-sig"),
            file_name="bili_dashboard_backup.csv",
            mime="text/csv"
        )
    uploaded_backup = st.file_uploader("å¯¼å…¥å¤‡ä»½CSVæ¢å¤", type=["csv"])
    if uploaded_backup is not None and st.button("ğŸ“¥ æ¢å¤å¤‡ä»½åˆ°æ•°æ®åº“"):
        raw = uploaded_backup.getvalue()
        df_imp = None
        for enc in ["utf-8-sig", "utf-8", "gbk"]:
            try:
                df_imp = pd.read_csv(io.BytesIO(raw), encoding=enc)
                break
            except Exception:
                df_imp = None
        if df_imp is None:
            st.error("æ¢å¤å¤±è´¥ï¼šCSVè¯»å–å¤±è´¥ï¼ˆå»ºè®®UTF-8ç¼–ç ï¼‰ã€‚")
        else:
            df_imp = normalize_df(df_imp)
            if "fetched_at" not in df_imp.columns:
                df_imp["fetched_at"] = pd.Timestamp.now()
            df_imp["pubdate"] = pd.to_datetime(df_imp["pubdate"], errors="coerce")
            df_imp["fetched_at"] = pd.to_datetime(df_imp["fetched_at"], errors="coerce").fillna(pd.Timestamp.now())
            df_imp["pubdate"] = df_imp["pubdate"].dt.strftime("%Y-%m-%d %H:%M:%S")
            df_imp["fetched_at"] = df_imp["fetched_at"].dt.strftime("%Y-%m-%d %H:%M:%S")
            upsert_rows(df_imp)
            st.success("æ¢å¤å®Œæˆã€‚")
            st.rerun()

with st.sidebar.expander("å±é™©æ“ä½œï¼šæ¸…ç©ºå…¨éƒ¨æ•°æ®", expanded=False):
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ•°æ®åº“ï¼ˆä¸å¯æ’¤é”€ï¼‰"):
        clear_all_data()
        st.success("å·²æ¸…ç©ºã€‚")
        st.rerun()

st.sidebar.divider()

# =========================
# Data input
# =========================
mode = st.sidebar.radio("æ•°æ®æ¥æº", ["ç²˜è´´é“¾æ¥/BVé‡‡é›†", "ä¸Šä¼ CSVå¯¼å…¥"], index=0)

if mode == "ç²˜è´´é“¾æ¥/BVé‡‡é›†":
    project = st.sidebar.text_input("é¡¹ç›®åï¼ˆç”¨äºå½’æ¡£ï¼‰", value="æœªå‘½åé¡¹ç›®")
    links = st.sidebar.text_area("ç²˜è´´è§†é¢‘é“¾æ¥/ BVå·ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰")
    add_btn = st.sidebar.button("â• é‡‡é›†å¹¶å…¥åº“ï¼ˆä¼šæ°¸ä¹…ä¿å­˜ï¼‰")

    if add_btn:
        items = [x for x in links.splitlines() if x.strip()]
        ok, fail = 0, 0
        rows = []
        for it in items:
            bvid = parse_bvid(it)
            if not bvid:
                fail += 1
                continue
            detail = fetch_video_detail_by_bvid(bvid)
            if detail is None:
                fail += 1
                continue
            detail["project"] = project
            detail["url"] = it
            detail["data_type"] = "collab"
            detail["baseline_for"] = ""
            detail["fans_delta"] = 0
            detail["fetched_at"] = pd.Timestamp.now()
            rows.append(detail)
            ok += 1
            time.sleep(0.35)

        if rows:
            df_new = normalize_df(pd.DataFrame(rows))
            df_new["pubdate"] = pd.to_datetime(df_new["pubdate"], errors="coerce").dt.strftime("%Y-%m-%d %H:%M:%S")
            df_new["fetched_at"] = pd.to_datetime(df_new["fetched_at"], errors="coerce").dt.strftime("%Y-%m-%d %H:%M:%S")
            upsert_rows(df_new)

        st.sidebar.success(f"æˆåŠŸé‡‡é›† {ok} æ¡ï¼Œå¤±è´¥ {fail} æ¡ï¼ˆå·²ä¿å­˜ï¼‰")
        st.rerun()

else:
    default_project = st.sidebar.text_input("ç¼ºå°‘ project åˆ—æ—¶ï¼šé»˜è®¤é¡¹ç›®å", value="æœªå‘½åé¡¹ç›®")
    uploaded = st.sidebar.file_uploader("é€‰æ‹©CSVæ–‡ä»¶", type=["csv"])
    import_btn = st.sidebar.button("ğŸ“¥ å¯¼å…¥CSVåˆ°ä»ªè¡¨ç›˜ï¼ˆä¼šæ°¸ä¹…ä¿å­˜ï¼‰")

    if import_btn:
        if not uploaded:
            st.sidebar.error("è¯·å…ˆé€‰æ‹©ä¸€ä¸ªCSVæ–‡ä»¶ã€‚")
        else:
            raw = uploaded.getvalue()
            df_csv = None
            for enc in ["utf-8-sig", "utf-8", "gbk"]:
                try:
                    df_csv = pd.read_csv(io.BytesIO(raw), encoding=enc)
                    break
                except Exception:
                    df_csv = None

            if df_csv is None:
                st.sidebar.error("CSVè¯»å–å¤±è´¥ï¼šå»ºè®®UTF-8ç¼–ç ã€‚")
            else:
                df_csv = normalize_df(df_csv)
                if "project" not in df_csv.columns:
                    df_csv["project"] = default_project
                df_csv["project"] = df_csv["project"].apply(lambda x: _safe_str(x).strip())
                df_csv.loc[df_csv["project"] == "", "project"] = default_project
                if "data_type" not in df_csv.columns:
                    df_csv["data_type"] = "collab"
                if "fetched_at" not in df_csv.columns:
                    df_csv["fetched_at"] = pd.Timestamp.now()

                df_csv["pubdate"] = pd.to_datetime(df_csv["pubdate"], errors="coerce").dt.strftime("%Y-%m-%d %H:%M:%S")
                df_csv["fetched_at"] = pd.to_datetime(df_csv["fetched_at"], errors="coerce").fillna(pd.Timestamp.now()).dt.strftime("%Y-%m-%d %H:%M:%S")
                upsert_rows(df_csv)

                st.sidebar.success(f"å¯¼å…¥æˆåŠŸï¼š{len(df_csv):,} è¡Œï¼ˆå·²ä¿å­˜ï¼‰")
                st.rerun()

# =========================
# Load data
# =========================
df_db = load_all_rows()
df_db = normalize_df(df_db) if not df_db.empty else df_db

st.title("Bç«™æ—¥å¸¸è¿è¥æ•°æ® Dashboard")
if df_db.empty:
    st.info("æ•°æ®åº“ä¸ºç©ºï¼šè¯·åœ¨å·¦ä¾§é‡‡é›†æˆ–å¯¼å…¥ã€‚")
    st.stop()

df_db = compute_metrics(df_db)

# =========================
# Project filter (hide baseline project)
# =========================
projects = sorted([p for p in df_db["project"].dropna().unique().tolist()
                   if str(p).strip() != "" and p != BASELINE_PROJECT])
sel_projects = st.sidebar.multiselect("é€‰æ‹©é¡¹ç›®ï¼ˆç­›é€‰å±•ç¤ºï¼‰", projects, default=projects if projects else None)

df_main = df_db[df_db["project"] != BASELINE_PROJECT].copy()
df_f = df_main[df_main["project"].isin(sel_projects)].copy() if sel_projects else df_main.copy()

# =========================
# å‘¨æŠ¥é¡¹ç›®é€‰æ‹©ï¼ˆæ–°å¢ï¼šåªé’ˆå¯¹å•é¡¹ç›®è¾“å‡ºï¼‰
# =========================
st.sidebar.markdown("#### å‘¨æŠ¥è¾“å‡º")
weekly_project = st.sidebar.selectbox(
    "é€‰æ‹©ä¸€ä¸ªé¡¹ç›®ç”¨äºå‘¨æŠ¥è§£è¯»ï¼ˆåªè¾“å‡ºè¯¥é¡¹ç›®ï¼‰",
    options=projects if projects else ["(æ— é¡¹ç›®)"],
    index=0 if projects else 0
)

# =========================
# Add performance labels (ç”¨äºè¡¨æ ¼/TopBottom)
# =========================
df_f = add_perf_cols(df_f, df_db, baseline_window_n, baseline_min_n)

# =========================
# KPI cards
# =========================
c1, c2, c3, c4 = st.columns(4)
c1.metric("æ€»æ’­æ”¾", f"{int(df_f['view'].sum()):,}")
c2.metric("æ€»äº’åŠ¨(èµ+å¸+è—+è¯„)", f"{int(df_f['engagement'].sum()):,}")
c3.metric("å¹³å‡äº’åŠ¨ç‡", f"{df_f['engagement_rate'].mean()*100:.2f}%")
c4.metric("æ·±åº¦ä¿¡å·å æ¯”(å¸+è—/äº’åŠ¨)", f"{df_f['deep_signal_ratio'].mean()*100:.1f}%")

# =========================
# Cross project comparison + Quadrant
# =========================
st.subheader("è·¨é¡¹ç›®å¯¹æ¯”ï¼ˆé¡¹ç›®ä¹‹é—´è°æ›´å¼ºã€è°æ›´ç¨³ï¼‰")
proj_rows = []
for proj, g in df_f.groupby("project"):
    g2 = g.sort_values("view", ascending=False).copy()
    total_view = int(g2["view"].sum())
    total_eng = int(g2["engagement"].sum())
    video_cnt = int(len(g2))
    up_cnt = int(g2["owner_name"].nunique())

    er_med = float(g2["engagement_rate"].median())
    deep_med = float(g2["deep_signal_ratio"].median())

    er_q1 = float(g2["engagement_rate"].quantile(0.25))
    er_q3 = float(g2["engagement_rate"].quantile(0.75))
    er_iqr = er_q3 - er_q1

    top1_view = int(g2.iloc[0]["view"]) if video_cnt > 0 else 0
    top3_view = int(g2.head(3)["view"].sum()) if video_cnt > 0 else 0
    top1_share = (top1_view / total_view) if total_view > 0 else 0.0
    top3_share = (top3_view / total_view) if total_view > 0 else 0.0

    proj_rows.append({
        "project": proj,
        "è§†é¢‘æ•°": video_cnt,
        "UPæ•°": up_cnt,
        "æ€»æ’­æ”¾": total_view,
        "æ€»äº’åŠ¨": total_eng,
        "äº’åŠ¨ç‡ä¸­ä½æ•°": er_med,
        "æ·±åº¦ä¿¡å·ä¸­ä½æ•°": deep_med,
        "äº’åŠ¨ç‡æ³¢åŠ¨(IQR)": er_iqr,
        "Top1æ’­æ”¾è´¡çŒ®": top1_share,
        "Top3æ’­æ”¾è´¡çŒ®": top3_share,
    })

proj_df = pd.DataFrame(proj_rows).sort_values("æ€»æ’­æ”¾", ascending=False)

st.dataframe(
    proj_df.assign(**{
        "äº’åŠ¨ç‡ä¸­ä½æ•°": (proj_df["äº’åŠ¨ç‡ä¸­ä½æ•°"]*100).map(lambda x: f"{x:.2f}%"),
        "æ·±åº¦ä¿¡å·ä¸­ä½æ•°": (proj_df["æ·±åº¦ä¿¡å·ä¸­ä½æ•°"]*100).map(lambda x: f"{x:.1f}%"),
        "äº’åŠ¨ç‡æ³¢åŠ¨(IQR)": (proj_df["äº’åŠ¨ç‡æ³¢åŠ¨(IQR)"]*100).map(lambda x: f"{x:.2f}pp"),
        "Top1æ’­æ”¾è´¡çŒ®": (proj_df["Top1æ’­æ”¾è´¡çŒ®"]*100).map(lambda x: f"{x:.1f}%"),
        "Top3æ’­æ”¾è´¡çŒ®": (proj_df["Top3æ’­æ”¾è´¡çŒ®"]*100).map(lambda x: f"{x:.1f}%"),
    }),
    use_container_width=True,
    height=260
)

st.markdown("**é¡¹ç›®å››è±¡é™ï¼ˆX=äº’åŠ¨ç‡ä¸­ä½æ•°ï¼ŒY=æ·±åº¦ä¿¡å·ä¸­ä½æ•°ï¼‰**")
if len(proj_df) >= 2:
    x_med = float(proj_df["äº’åŠ¨ç‡ä¸­ä½æ•°"].median())
    y_med = float(proj_df["æ·±åº¦ä¿¡å·ä¸­ä½æ•°"].median())

    fig_q = px.scatter(
        proj_df,
        x="äº’åŠ¨ç‡ä¸­ä½æ•°",
        y="æ·±åº¦ä¿¡å·ä¸­ä½æ•°",
        size="æ€»æ’­æ”¾",
        text="project",
        hover_data=["è§†é¢‘æ•°","UPæ•°","æ€»æ’­æ”¾","Top1æ’­æ”¾è´¡çŒ®","Top3æ’­æ”¾è´¡çŒ®","äº’åŠ¨ç‡æ³¢åŠ¨(IQR)"],
    )
    fig_q.add_vline(x=x_med, line_dash="dash")
    fig_q.add_hline(y=y_med, line_dash="dash")
    fig_q.update_traces(textposition="top center")
    fig_q.update_layout(xaxis_tickformat=".0%", yaxis_tickformat=".0%")
    st.plotly_chart(fig_q, use_container_width=True)

# =========================
# âœ… æ–°å¢ï¼šå››è±¡é™ä¸‹æ–¹çš„â€œè·¨é¡¹ç›®è§£è¯»â€
# =========================
st.subheader("è·¨é¡¹ç›®è§£è¯»ï¼ˆæ”¾åœ¨å››è±¡é™ä¸‹é¢ï¼Œä¾¿äºå‘¨æŠ¥å¼•ç”¨ï¼‰")
if proj_df.empty:
    st.info("æš‚æ— é¡¹ç›®æ•°æ®å¯è§£è¯»ã€‚")
else:
    # å–å‡ ä¸ªç»´åº¦ç”¨äºæè¿°â€œå¼º/ç¨³/é£é™©â€
    p = proj_df.copy()
    p["er"] = p["äº’åŠ¨ç‡ä¸­ä½æ•°"]
    p["deep"] = p["æ·±åº¦ä¿¡å·ä¸­ä½æ•°"]
    p["iqr"] = p["äº’åŠ¨ç‡æ³¢åŠ¨(IQR)"]
    p["top1"] = p["Top1æ’­æ”¾è´¡çŒ®"]
    p["top3"] = p["Top3æ’­æ”¾è´¡çŒ®"]

    # æ›´å¼ºï¼šäº’åŠ¨ç‡&æ·±åº¦éƒ½é«˜ï¼ˆå››è±¡é™å³ä¸Šï¼‰
    # æ›´ç¨³ï¼šIQRå°
    # é£é™©ï¼šTop1è´¡çŒ®è¿‡é«˜ / IQRè¿‡å¤§
    strongest = p.sort_values(["er","deep"], ascending=False).head(1).iloc[0]
    steadiest = p.sort_values(["iqr","er"], ascending=[True, False]).head(1).iloc[0]
    risky = p.sort_values(["top1","iqr"], ascending=False).head(1).iloc[0]

    lines = []
    lines.append(f"1ï¼‰**æ•´ä½“ç»“æ„**ï¼šå½“å‰é¡¹ç›®åœ¨å››è±¡é™ä¸­å‘ˆç°â€œå·®å¼‚åŒ–åˆ†å¸ƒâ€â€”â€”æ—¢æœ‰åäº’åŠ¨å‹é¡¹ç›®ï¼Œä¹Ÿæœ‰åæ²‰æ·€å‹é¡¹ç›®ï¼Œé€‚åˆé‡‡ç”¨ä¸åŒçš„å†…å®¹æ‰“æ³•ä¸ç›®æ ‡KPIã€‚")
    lines.append(f"2ï¼‰**æ›´å¼ºé¡¹ç›®ï¼ˆäº’åŠ¨&æ²‰æ·€ç»¼åˆæ›´é å‰ï¼‰**ï¼š{strongest['project']}ï¼ˆäº’åŠ¨ç‡ä¸­ä½æ•° {strongest['er']*100:.2f}%ï¼Œæ·±åº¦ä¿¡å·ä¸­ä½æ•° {strongest['deep']*100:.1f}%ï¼‰ï¼Œå»ºè®®å»¶ç»­è¯¥é¡¹ç›®çš„é€‰é¢˜/åŒ…è£…æ–¹å¼å¹¶è¿›ä¸€æ­¥æ¨¡æ¿åŒ–ã€‚")
    lines.append(f"3ï¼‰**æ›´ç¨³é¡¹ç›®ï¼ˆæ³¢åŠ¨æ›´å°ï¼‰**ï¼š{steadiest['project']}ï¼ˆäº’åŠ¨ç‡æ³¢åŠ¨IQR {steadiest['iqr']*100:.2f}ppï¼‰ï¼Œè¯´æ˜è¾“å‡ºä¸€è‡´æ€§æ›´å¼ºï¼Œé€‚åˆç¨³å®šèŠ‚å¥/æŒç»­æŠ•æ”¾ä¸ç³»åˆ—åŒ–ã€‚")
    lines.append(f"4ï¼‰**ç»“æ„é£é™©æç¤º**ï¼š{risky['project']} çš„Top1æ’­æ”¾è´¡çŒ® {risky['top1']*100:.1f}%ï¼ˆTop3è´¡çŒ® {risky['top3']*100:.1f}%ï¼‰ï¼Œå­˜åœ¨â€œå¤´éƒ¨ä¾èµ–â€å€¾å‘ï¼Œå»ºè®®è¡¥é½è…°éƒ¨å†…å®¹å¯†åº¦ï¼Œé™ä½å•ç‚¹æ³¢åŠ¨ã€‚")

    st.write("\n".join(lines))

# =========================
# é¡¹ç›®å†…è§†é¢‘è¡¨
# =========================
st.divider()
st.subheader("é¡¹ç›®å†…è§†é¢‘è¡¨ç°ï¼ˆæŒ‰æ’­æ”¾æ’åºï¼‰")
show_cols = [
    "project","bvid","title","owner_name","pubdate",
    "view","æ’­æ”¾è¡¨ç°",
    "engagement_rate","äº’åŠ¨ç‡è¡¨ç°",
    "like","coin","favorite","reply",
    "deep_signal_ratio"
]
st.dataframe(df_f[show_cols].sort_values("view", ascending=False), use_container_width=True, height=360)

# =========================
# Top/Bottom æ·±æŒ–
# =========================
st.subheader("Top / Bottom æ·±æŒ–ï¼ˆå«KOLè‡ªèº«åŸºå‡†åˆ¤æ–­ï¼‰")
for proj in (sel_projects if sel_projects else projects):
    d = df_f[df_f["project"] == proj].sort_values("view", ascending=False)
    if d.empty:
        continue
    top = d.iloc[0]
    bottom = d.iloc[-1]

    st.markdown(f"### é¡¹ç›®ï¼š{proj}")
    left, right = st.columns(2)

    def render_card(col, row, tag):
        col.markdown(f"**{tag}ï¼š{row['title']}**")
        col.caption(f"UPï¼š{row['owner_name']} ï½œ BVï¼š{row['bvid']} ï½œ å‘å¸ƒï¼š{row['pubdate']}")
        col.metric("æ’­æ”¾", f"{int(row['view']):,}", row["æ’­æ”¾è¡¨ç°"])
        col.metric("äº’åŠ¨ç‡", f"{row['engagement_rate']*100:.2f}%", row["äº’åŠ¨ç‡è¡¨ç°"])
        col.write(f"- èµ/å¸/è—/è¯„ï¼š{int(row['like'])}/{int(row['coin'])}/{int(row['favorite'])}/{int(row['reply'])}")
        col.write(f"- æ·±åº¦ä¿¡å·å æ¯”ï¼š{row['deep_signal_ratio']*100:.1f}%")

    render_card(left, top, "ğŸ”¥ æœ€é«˜æ’­æ”¾")
    render_card(right, bottom, "ğŸ§Š æœ€ä½æ’­æ”¾")

# =========================
# ç®±çº¿å›¾
# =========================
st.subheader("äº’åŠ¨ç‡åˆ†å¸ƒï¼ˆé¡¹ç›®/UPä¸»å¿«é€Ÿå®šä½å¼‚å¸¸ï¼‰")
fig = px.box(df_f, x="project", y="engagement_rate", points="all", hover_data=["title","owner_name","view"])
st.plotly_chart(fig, use_container_width=True)

# =========================
# âœ… æ–°å¢ï¼šå‘¨æŠ¥è§£è¯»ï¼ˆåªé’ˆå¯¹å•ä¸ªé¡¹ç›®ï¼‰
# =========================
st.subheader("å‘¨æŠ¥è§£è¯»ï¼ˆåªè¾“å‡ºå•ä¸ªé¡¹ç›®ï¼Œå¯ç›´æ¥å¤åˆ¶ï¼‰")

if weekly_project and weekly_project in df_f["project"].unique():
    wk = df_f[df_f["project"] == weekly_project].copy()
    wk = wk.sort_values("view", ascending=False)
    total_view = int(wk["view"].sum())
    total_eng = int(wk["engagement"].sum())
    er_med = float(wk["engagement_rate"].median())
    deep_med = float(wk["deep_signal_ratio"].median())
    video_cnt = int(len(wk))
    up_cnt = int(wk["owner_name"].nunique())

    top = wk.iloc[0]
    bottom = wk.iloc[-1]

    # é£é™©/ç»“æ„ï¼šå¤´éƒ¨ä¾èµ–
    top1_share = float(top["view"]) / total_view if total_view > 0 else 0.0
    top3_share = float(wk.head(3)["view"].sum()) / total_view if total_view > 0 else 0.0

    # æ³¢åŠ¨ï¼šäº’åŠ¨ç‡IQR
    er_iqr = float(wk["engagement_rate"].quantile(0.75) - wk["engagement_rate"].quantile(0.25))

    out = []
    out.append(f"ã€é¡¹ç›®ï¼š{weekly_project}ã€‘æœ¬æœŸå…±äº§å‡º {video_cnt} æ¡å†…å®¹ï¼Œè¦†ç›– {up_cnt} ä½UPï¼Œç´¯è®¡æ’­æ”¾ {total_view:,}ï¼Œç´¯è®¡äº’åŠ¨ {total_eng:,}ã€‚")
    out.append(f"äº’åŠ¨è´¨é‡ä¿æŒç¨³å®šï¼šäº’åŠ¨ç‡ä¸­ä½æ•° {er_med*100:.2f}%ï¼ˆæ³¢åŠ¨IQR {er_iqr*100:.2f}ppï¼‰ï¼Œæ·±åº¦ä¿¡å·ä¸­ä½æ•° {deep_med*100:.1f}%ï¼ˆå¸+è—å äº’åŠ¨æ¯”ï¼‰ã€‚")
    out.append(f"æœ€é«˜æ’­æ”¾ç”±ã€Š{top['title']}ã€‹è´¡çŒ®ï¼ˆ{int(top['view']):,} æ’­æ”¾ï¼Œäº’åŠ¨ç‡ {top['engagement_rate']*100:.2f}%ï¼‰ï¼ŒéªŒè¯è¯¥é€‰é¢˜/åŒ…è£…å…·å¤‡å¯å¤åˆ¶çš„æµé‡æŠ“æ‰‹ã€‚")
    out.append(f"æœ€ä½æ’­æ”¾ã€Š{bottom['title']}ã€‹ä¸º {int(bottom['view']):,} æ’­æ”¾ï¼ˆäº’åŠ¨ç‡ {bottom['engagement_rate']*100:.2f}%ï¼‰ï¼Œå»ºè®®åœ¨å°é¢/æ ‡é¢˜ä¿¡æ¯å¯†åº¦ä¸è¯„è®ºåŒºäº’åŠ¨å¼•å¯¼ä¸Šåšè½»é‡ä¼˜åŒ–ï¼Œæå‡åŸºç¡€ç›˜ã€‚")
    out.append(f"ç»“æ„è§‚å¯Ÿï¼šTop1æ’­æ”¾è´¡çŒ® {top1_share*100:.1f}%ï¼ˆTop3è´¡çŒ® {top3_share*100:.1f}%ï¼‰ï¼Œåç»­å°†é€šè¿‡å¤ç”¨é«˜è¡¨ç°æ¨¡æ¿+è¡¥é½è…°éƒ¨å†…å®¹ï¼Œé™ä½å•ç‚¹æ³¢åŠ¨ã€ç¨³å®šæ”¾å¤§é¡¹ç›®äº§å‡ºã€‚")

    st.write("\n".join(out))
else:
    st.info("è¯·é€‰æ‹©ä¸€ä¸ªæœ‰æ•ˆé¡¹ç›®ç”¨äºå‘¨æŠ¥è¾“å‡ºã€‚")

# =========================
# ä¿ç•™ï¼šå…¨å±€è‡ªåŠ¨è§£è¯»ï¼ˆåŸæ¨¡å—ä¸åˆ ï¼‰
# =========================
st.subheader("å…¨å±€è‡ªåŠ¨è§£è¯»ï¼ˆåŸæ¨¡å—ä¿ç•™ï¼‰")
best = df_f.sort_values("view", ascending=False).iloc[0]
worst = df_f.sort_values("view", ascending=True).iloc[0]
insights = []
insights.append(
    f"1ï¼‰æœ¬æœŸæœ€é«˜æ’­æ”¾æ¥è‡ªã€Š{best['title']}ã€‹ï¼ˆ{int(best['view']):,} æ’­æ”¾ï¼Œ{best['æ’­æ”¾è¡¨ç°']}ï¼‰ï¼Œäº’åŠ¨ç‡ {best['engagement_rate']*100:.2f}%ï¼ˆ{best['äº’åŠ¨ç‡è¡¨ç°']}ï¼‰ã€‚"
)
insights.append(
    f"2ï¼‰æœ€ä½æ’­æ”¾ä¸ºã€Š{worst['title']}ã€‹ï¼ˆ{int(worst['view']):,} æ’­æ”¾ï¼Œ{worst['æ’­æ”¾è¡¨ç°']}ï¼‰ï¼Œäº’åŠ¨ç‡ {worst['engagement_rate']*100:.2f}%ï¼ˆ{worst['äº’åŠ¨ç‡è¡¨ç°']}ï¼‰ã€‚å»ºè®®æ£€æŸ¥å°é¢/æ ‡é¢˜ä¿¡æ¯å¯†åº¦ä¸æŠ•æ”¾æ—¶æ®µï¼Œå¹¶åœ¨è¯„è®ºåŒºåšæ›´å¼ºçš„äº’åŠ¨å¼•å¯¼ã€‚"
)
if df_f["deep_signal_ratio"].mean() < 0.35:
    insights.append("3ï¼‰æ•´ä½“æ·±åº¦ä¿¡å·åä½ï¼ˆå¸+è—åœ¨äº’åŠ¨ä¸­çš„å æ¯”ä¸é«˜ï¼‰ï¼Œè¯´æ˜å†…å®¹æ›´å¤šæ˜¯â€œè·¯è¿‡å‹çƒ­åº¦â€ï¼Œå»ºè®®å¼ºåŒ–ï¼šä»·å€¼ç‚¹å‰ç½®ã€ç»“å°¾å¼•å¯¼æ”¶è—/æŠ•å¸ã€å¢åŠ ç³»åˆ—åŒ–æ‰¿è¯ºã€‚")
else:
    insights.append("3ï¼‰æ•´ä½“æ·±åº¦ä¿¡å·å¥åº·ï¼ˆå¸+è—å æ¯”é«˜ï¼‰ï¼Œè¯´æ˜å†…å®¹å…·å¤‡æ²‰æ·€å±æ€§ï¼Œå¯è€ƒè™‘å›´ç»•è¯¥æ–¹å‘åšç³»åˆ—åŒ–ä¸å›ºå®šæ ç›®èŠ‚å¥ã€‚")
st.write("\n".join(insights))

# =========================================================
# KOL moduleï¼ˆæŒ‰ owner_midï¼Œå¯¹é½+è¡¥é½+æ ‡æ³¨+å¯¼å‡ºï¼‰
# =========================================================
st.divider()
st.subheader("KOLåˆä½œèµ„æ–™åº“ï¼ˆç‹¬ç«‹æ¨¡å—ï¼šæ ‡æ³¨åˆä½œæ˜¯å¦ä¼˜äºå¹³æ—¶ï½œæŒ‰owner_midå¯¹é½ï¼‰")

with st.expander("KOLæ¨¡å—è®¾ç½®", expanded=False):
    collab_projects = st.multiselect("å“ªäº›é¡¹ç›®ç®—åˆä½œé¡¹ç›®", projects, default=sel_projects if sel_projects else projects)
    fetch_n = st.slider("è¡¥é½åŸºå‡†ï¼šæ¯ä¸ªKOLæŠ“å–æœ€è¿‘Næ¡å…¬å¼€è§†é¢‘", 10, 80, 30, step=5)
    sleep_sec = st.slider("æŠ“å–é—´éš”ï¼ˆé˜²é™æµï¼‰", 0.2, 2.0, 0.8, step=0.1)

cA, cB, cC = st.columns([1, 1, 2])
with cA:
    btn_fill_all = st.button("ğŸ§² ä¸€é”®è¡¥é½æ‰€æœ‰åˆä½œKOLåŸºå‡†ï¼ˆå†™å…¥__BASELINE__ï¼‰")
with cB:
    btn_build_kol = st.button("ğŸ“š ç”ŸæˆKOLå¯¹æ¯”è¡¨ï¼ˆå«æ ‡æ³¨ï¼‰")
with cC:
    st.caption("æ ‡æ³¨ï¼šâ­åˆä½œæ˜æ˜¾æ›´å¥½ / âš ï¸åˆä½œåå¼± / ç©º=æ­£å¸¸åŒºé—´ã€‚")

if collab_projects:
    collab_df = df_db[df_db["project"].isin(collab_projects)].copy()
    collab_df["owner_mid"] = collab_df["owner_mid"].apply(_norm_mid)

    valid_mid_df = collab_df[collab_df["owner_mid"].astype(str).str.len() > 0].copy()
    invalid_mid_cnt = int((collab_df["owner_mid"].astype(str).str.len() == 0).sum())

    st.caption(f"åˆä½œUPä¸»æ•°ï¼š{collab_df['owner_mid'].nunique()}ï¼ˆå«ç¼º/å¼‚å¸¸midï¼‰ï½œå¯æŠ“å–midçš„UPæ•°ï¼š{valid_mid_df['owner_mid'].nunique()}ï½œåˆä½œè§†é¢‘æ•°ï¼š{len(collab_df)}")
    if invalid_mid_cnt > 0:
        st.warning(f"æœ‰ {invalid_mid_cnt} æ¡åˆä½œè§†é¢‘ owner_mid ç¼ºå¤±æˆ–å¼‚å¸¸ï¼ˆè¶…é•¿/éæ•°å­—ï¼‰ã€‚å»ºè®®ä¿®CSV mid æˆ–ç”¨BVé‡‡é›†è¡¥é½ã€‚")

    name_map = (valid_mid_df.groupby("owner_mid")["owner_name"]
                .agg(lambda s: s.value_counts().index[0]).to_dict())

    if btn_fill_all:
        # âœ… åªæŒ‰ baseline é¡¹ç›®å»é‡ï¼šåŒä¸€ä¸ªBVå…è®¸åŒæ—¶å­˜åœ¨äºåˆä½œé¡¹ç›®å’Œ __BASELINE__
        existed_baseline = set(df_db[df_db["project"] == BASELINE_PROJECT]["bvid"].astype(str).tolist())

        rows_to_write = {}  # key=(project,bvid) -> row dict
        stat = {"list_fail": 0, "detail_ok": 0, "detail_fail": 0, "vlist_added": 0}

        for mid in sorted(valid_mid_df["owner_mid"].unique().tolist()):
            try:
                vlist = fetch_vlist_by_mid(int(mid), n=int(fetch_n))
            except Exception:
                stat["list_fail"] += 1
                continue

            disp = name_map.get(mid, "")

            for v in vlist:
                bvid = v["bvid"]
                if bvid in existed_baseline:
                    continue

                base_row = {
                    "project": BASELINE_PROJECT,
                    "bvid": bvid,
                    "url": f"https://www.bilibili.com/video/{bvid}",
                    "title": v.get("title",""),
                    "pubdate": v.get("pubdate", pd.NaT),
                    "owner_mid": mid,
                    "owner_name": disp,
                    "view": _safe_int(v.get("view",0)),
                    "reply": _safe_int(v.get("reply",0)),
                    "like": 0, "coin": 0, "favorite": 0, "danmaku": 0, "share": 0,
                    "fans_delta": 0,
                    "baseline_for": disp,
                    "data_type": "baseline",
                    "fetched_at": pd.Timestamp.now(),
                }
                rows_to_write[(BASELINE_PROJECT, bvid)] = base_row
                existed_baseline.add(bvid)
                stat["vlist_added"] += 1

                detail = fetch_video_detail_by_bvid(bvid)
                if detail is not None:
                    detail["project"] = BASELINE_PROJECT
                    detail["url"] = f"https://www.bilibili.com/video/{bvid}"
                    detail["baseline_for"] = disp
                    detail["data_type"] = "baseline"
                    detail["fans_delta"] = 0
                    detail["fetched_at"] = pd.Timestamp.now()
                    rows_to_write[(BASELINE_PROJECT, bvid)] = detail
                    stat["detail_ok"] += 1
                else:
                    stat["detail_fail"] += 1

                time.sleep(float(sleep_sec))

        if rows_to_write:
            df_new = normalize_df(pd.DataFrame(list(rows_to_write.values())))
            df_new["pubdate"] = pd.to_datetime(df_new["pubdate"], errors="coerce").dt.strftime("%Y-%m-%d %H:%M:%S")
            df_new["fetched_at"] = pd.to_datetime(df_new["fetched_at"], errors="coerce").dt.strftime("%Y-%m-%d %H:%M:%S")
            upsert_rows(df_new)
            st.success(f"è¡¥é½å®Œæˆï¼šæ–°å¢ {stat['vlist_added']} æ¡åŸºå‡†ï¼›è¯¦æƒ…è¡¥å…¨æˆåŠŸ {stat['detail_ok']}ï¼Œå¤±è´¥ {stat['detail_fail']}ï¼›åˆ—è¡¨å¤±è´¥ {stat['list_fail']}")
            st.rerun()
        else:
            st.warning("æœ¬æ¬¡æœªæ–°å¢ï¼šå¯èƒ½å·²è¡¥é½ã€æˆ–æ¥å£æ³¢åŠ¨å¯¼è‡´vlistä¸ºç©ºã€‚")

    st.markdown("**KOLåŸºå‡†è¯Šæ–­ï¼ˆæŒ‰owner_midç»Ÿè®¡åº“å†…æ•°é‡ï¼‰**")
    diag = []
    for mid in sorted(valid_mid_df["owner_mid"].unique().tolist()):
        owner_all = df_db[df_db["owner_mid"].apply(_norm_mid) == mid].copy()
        owner_all = _sort_owner_hist(owner_all)

        base_pool = owner_all[~owner_all["project"].isin(set(collab_projects))].copy()
        base_pool = pd.concat([base_pool, owner_all[owner_all["project"] == BASELINE_PROJECT]], ignore_index=True)
        base_pool = base_pool.drop_duplicates(subset=["bvid"], keep="last")
        base_pool = _sort_owner_hist(base_pool)

        avail = int(min(len(base_pool), baseline_window_n))
        diag.append({
            "owner_mid": mid,
            "KOL/UPä¸»": name_map.get(mid, owner_all["owner_name"].dropna().iloc[0] if not owner_all.empty else ""),
            "åº“å†…è§†é¢‘æ€»æ•°": int(len(owner_all)),
            "å¯ç”¨åŸºå‡†æ•°(å¹³æ—¶æ± )": int(len(base_pool)),
            f"å–æœ€è¿‘{baseline_window_n}å¯ç”¨": avail,
            "çŠ¶æ€": "OK" if avail >= baseline_min_n else f"åŸºå‡†ä¸è¶³(<{baseline_min_n})",
        })
    st.dataframe(pd.DataFrame(diag).sort_values(["çŠ¶æ€","å¯ç”¨åŸºå‡†æ•°(å¹³æ—¶æ± )"], ascending=[True, False]),
                 use_container_width=True, height=360)

    if btn_build_kol:
        df_all_m = compute_metrics(df_db.copy())
        df_all_m["owner_mid"] = df_all_m["owner_mid"].apply(_norm_mid)
        rows = []

        collab_mid_df = df_all_m[df_all_m["project"].isin(collab_projects)].copy()
        collab_mid_df = collab_mid_df[collab_mid_df["owner_mid"].astype(str).str.len() > 0]

        for mid, g_collab in collab_mid_df.groupby("owner_mid"):
            up_name = (g_collab["owner_name"].value_counts().index[0]
                       if not g_collab["owner_name"].dropna().empty else name_map.get(mid, ""))

            owner_all = df_all_m[df_all_m["owner_mid"] == mid].copy()
            owner_all = _sort_owner_hist(owner_all)

            base_pool = owner_all[~owner_all["project"].isin(set(collab_projects))].copy()
            base_pool = pd.concat([base_pool, owner_all[owner_all["project"] == BASELINE_PROJECT]], ignore_index=True)
            base_pool = base_pool.drop_duplicates(subset=["bvid"], keep="last")
            base_pool = _sort_owner_hist(base_pool).head(baseline_window_n)

            if len(base_pool) < baseline_min_n:
                continue

            base_view = float(base_pool["view"].median())
            base_er = float(base_pool["engagement_rate"].median())
            base_deep = float(base_pool["deep_signal_ratio"].median())

            collab_view = float(g_collab["view"].median())
            collab_er = float(g_collab["engagement_rate"].median())
            collab_deep = float(g_collab["deep_signal_ratio"].median())

            view_lift = (collab_view / base_view - 1.0) if base_view > 0 else np.nan
            er_lift = (collab_er / base_er - 1.0) if base_er > 0 else np.nan
            deep_lift = (collab_deep / base_deep - 1.0) if base_deep > 0 else np.nan

            mark = kol_flag(view_lift, er_lift, deep_lift)

            tags = []
            if not np.isnan(view_lift) and view_lift >= 0.30: tags.append("çƒ­åº¦æ‹‰å‡")
            if not np.isnan(er_lift) and er_lift >= 0.20: tags.append("äº’åŠ¨å¢å¼º")
            if not np.isnan(deep_lift) and deep_lift >= 0.10: tags.append("æ²‰æ·€æå‡")
            if not tags: tags.append("å¸¸è§„")

            persona = f"{'çƒ­åº¦æ‹‰å‡' if 'çƒ­åº¦æ‹‰å‡' in tags else 'çƒ­åº¦ç¨³å®š'} + {'äº’åŠ¨å¢å¼º' if 'äº’åŠ¨å¢å¼º' in tags else 'äº’åŠ¨å¸¸è§„'} + {'æ²‰æ·€æå‡' if 'æ²‰æ·€æå‡' in tags else 'æ²‰æ·€ä¸€èˆ¬'}"

            rows.append({
                "owner_mid": mid,
                "KOL/UPä¸»": up_name,
                "æ ‡æ³¨": mark,
                "åˆä½œè§†é¢‘æ•°": int(len(g_collab)),
                "åŸºå‡†æ ·æœ¬æ•°": int(len(base_pool)),
                "æ ‡ç­¾": "ã€".join(tags),
                "KOLç”»åƒä¸€å¥è¯": persona,
                "åˆä½œæ’­æ”¾ä¸­ä½æ•°": int(collab_view),
                "åŸºå‡†æ’­æ”¾ä¸­ä½æ•°": int(base_view),
                "æ’­æ”¾æå‡": "-" if np.isnan(view_lift) else f"{view_lift*100:.1f}%",
                "åˆä½œäº’åŠ¨ç‡ä¸­ä½æ•°": f"{collab_er*100:.2f}%",
                "åŸºå‡†äº’åŠ¨ç‡ä¸­ä½æ•°": f"{base_er*100:.2f}%",
                "äº’åŠ¨ç‡æå‡": "-" if np.isnan(er_lift) else f"{er_lift*100:.1f}%",
                "åˆä½œæ·±åº¦ä¿¡å·ä¸­ä½æ•°": f"{collab_deep*100:.1f}%",
                "åŸºå‡†æ·±åº¦ä¿¡å·ä¸­ä½æ•°": f"{base_deep*100:.1f}%",
                "æ·±åº¦ä¿¡å·æå‡": "-" if np.isnan(deep_lift) else f"{deep_lift*100:.1f}%"
            })

        if not rows:
            st.warning("æ²¡æœ‰ç”ŸæˆKOLç»“æœï¼šè¯·å…ˆè¡¥é½åŸºå‡†ï¼Œæˆ–é™ä½æœ€ä½æ ·æœ¬æ•°ã€‚")
        else:
            lib = pd.DataFrame(rows)

            def _pct_to_float(x):
                try:
                    if x == "-" or pd.isna(x):
                        return -999
                    return float(str(x).replace("%",""))
                except Exception:
                    return -999

            lib["_flag"] = lib["æ ‡æ³¨"].apply(lambda s: 2 if str(s).startswith("â­") else (1 if str(s).startswith("âš ï¸") else 0))
            lib["_view"] = lib["æ’­æ”¾æå‡"].map(_pct_to_float)
            lib["_er"] = lib["äº’åŠ¨ç‡æå‡"].map(_pct_to_float)
            lib = lib.sort_values(["_flag","_view","_er"], ascending=[False, False, False]).drop(columns=["_flag","_view","_er"])

            st.dataframe(lib, use_container_width=True, height=520)
            st.download_button(
                "â¬‡ï¸ ä¸‹è½½KOLå¯¹æ¯”è¡¨ï¼ˆCSVï¼‰",
                data=lib.to_csv(index=False).encode("utf-8-sig"),
                file_name="kol_compare.csv",
                mime="text/csv"
            )
