import re
import time
import io
import requests
import numpy as np
import pandas as pd
import streamlit as st
import plotly.express as px

st.set_page_config(page_title="Bç«™è¿è¥æ•°æ®Dashboard", layout="wide")

# =========================
# Utils
# =========================
REQUIRED_COLS_MIN = ["bvid", "title", "owner_name", "pubdate", "view", "like", "coin", "favorite", "reply", "project"]

NUM_COLS = ["view", "like", "coin", "favorite", "reply", "danmaku", "share", "fans_delta"]
OPTIONAL_COLS = ["danmaku", "share", "fans_delta", "owner_mid", "fetched_at", "url"]

def parse_bvid(url_or_bv: str) -> str | None:
    s = (url_or_bv or "").strip()
    m = re.search(r"(BV[0-9A-Za-z]{10})", s)
    return m.group(1) if m else None

def _safe_int(x, default=0):
    try:
        if pd.isna(x):
            return default
        if isinstance(x, str):
            x = x.replace(",", "").strip()
        return int(float(x))
    except Exception:
        return default

def _safe_str(x, default=""):
    try:
        if pd.isna(x):
            return default
        return str(x)
    except Exception:
        return default

def _safe_date(x):
    # Accept: YYYY-MM-DD, YYYY/MM/DD, timestamp, etc.
    try:
        if pd.isna(x):
            return pd.NaT
        return pd.to_datetime(x, errors="coerce")
    except Exception:
        return pd.NaT

def normalize_df(df: pd.DataFrame) -> pd.DataFrame:
    """Standardize columns + dtypes, and ensure required columns exist."""
    df = df.copy()

    # normalize column names: strip, lower
    df.columns = [str(c).strip() for c in df.columns]
    col_map_lower = {c.lower(): c for c in df.columns}

    # Make sure required columns exist (case-insensitive)
    def pick(col):
        return col_map_lower.get(col, None)

    # Try to map common Chinese headers to our standard schema
    zh_alias = {
        "é¡¹ç›®": "project",
        "é¡¹ç›®å": "project",
        "è§†é¢‘é“¾æ¥": "url",
        "é“¾æ¥": "url",
        "æ ‡é¢˜": "title",
        "UPä¸»": "owner_name",
        "UPä¸»åç§°": "owner_name",
        "å‘å¸ƒæ—¶é—´": "pubdate",
        "æ’­æ”¾": "view",
        "æ’­æ”¾é‡": "view",
        "ç‚¹èµ": "like",
        "æŠ•å¸": "coin",
        "æ”¶è—": "favorite",
        "è¯„è®º": "reply",
        "å¼¹å¹•": "danmaku",
        "åˆ†äº«": "share",
        "ç²‰ä¸å¢é•¿": "fans_delta",
        "ç²‰ä¸å¢é‡": "fans_delta",
        "BV": "bvid",
        "BVå·": "bvid",
        "bvid": "bvid",
    }

    # Build rename dict
    rename = {}
    for c in df.columns:
        key = str(c).strip()
        if key in zh_alias:
            rename[c] = zh_alias[key]
        else:
            # also handle lowercase match
            low = key.lower()
            if low in ["project","url","bvid","title","owner_name","owner_mid","pubdate",
                       "view","like","coin","favorite","reply","danmaku","share","fans_delta","fetched_at"]:
                rename[c] = low

    df = df.rename(columns=rename)

    # If url exists but bvid missing, try parse
    if "bvid" not in df.columns and "url" in df.columns:
        df["bvid"] = df["url"].apply(parse_bvid)

    # If bvid exists but has URLs inside, parse them
    if "bvid" in df.columns:
        df["bvid"] = df["bvid"].apply(lambda x: parse_bvid(x) if isinstance(x, str) else x)
        df["bvid"] = df["bvid"].apply(lambda x: _safe_str(x))

    # Ensure required text cols exist
    for col in ["project", "title", "owner_name"]:
        if col not in df.columns:
            df[col] = ""

    # Parse dates
    if "pubdate" not in df.columns:
        df["pubdate"] = pd.NaT
    df["pubdate"] = df["pubdate"].apply(_safe_date)

    # Ensure numeric cols exist
    for col in NUM_COLS:
        if col not in df.columns:
            df[col] = 0
        df[col] = df[col].apply(_safe_int)

    # Ensure fetched_at exists
    if "fetched_at" not in df.columns:
        df["fetched_at"] = pd.Timestamp.now()
    else:
        df["fetched_at"] = pd.to_datetime(df["fetched_at"], errors="coerce")
        df["fetched_at"] = df["fetched_at"].fillna(pd.Timestamp.now())

    # Keep only known cols + required
    keep = set(["project","bvid","url","title","pubdate","owner_mid","owner_name",
                "view","like","coin","favorite","reply","danmaku","share","fans_delta","fetched_at"])
    existing = [c for c in df.columns if c in keep]
    df = df[existing].copy()

    # Drop rows without bvid/title (best effort)
    if "bvid" in df.columns:
        df = df[df["bvid"].astype(str).str.startswith("BV")]

    return df

def compute_metrics(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df["engagement"] = df["like"] + df["coin"] + df["favorite"] + df["reply"]
    df["engagement_rate"] = np.where(df["view"] > 0, df["engagement"] / df["view"], 0.0)
    df["coin_rate"] = np.where(df["view"] > 0, df["coin"] / df["view"], 0.0)
    df["fav_rate"] = np.where(df["view"] > 0, df["favorite"] / df["view"], 0.0)
    df["reply_rate"] = np.where(df["view"] > 0, df["reply"] / df["view"], 0.0)
    df["deep_signal_ratio"] = np.where(
        df["engagement"] > 0, (df["coin"] + df["favorite"]) / df["engagement"], 0.0
    )
    return df

def label_vs_baseline(value: float, baseline_mean: float, baseline_std: float) -> str:
    if baseline_std <= 1e-9 or np.isnan(baseline_std):
        return "æ­£å¸¸å‘æŒ¥"
    z = (value - baseline_mean) / baseline_std
    if z >= 1.0:
        return "è¶…å¸¸å‘æŒ¥"
    if z <= -1.0:
        return "ä½äºé¢„æœŸ"
    return "æ­£å¸¸å‘æŒ¥"

# =========================
# Bç«™æŠ“å–ï¼ˆé“¾æ¥/BVé‡‡é›†ï¼‰
# =========================
def fetch_video_stats_by_bvid(bvid: str) -> dict:
    """
    è¿”å›å­—æ®µï¼štitle, pubdate, owner_mid, owner_name, view, like, coin, favorite, reply, danmaku, share
    æ³¨æ„ï¼šæ¥å£å¯å˜ï¼›å¤±è´¥ä¼šæŠ›é”™ï¼Œå‰ç«¯æç¤ºæ”¹ç”¨CSVå¯¼å…¥å…œåº•ã€‚
    """
    api = "https://api.bilibili.com/x/web-interface/view"
    headers = {"User-Agent": "Mozilla/5.0"}  # âœ… æå‡ç¨³å®šæ€§

    r = requests.get(api, params={"bvid": bvid}, headers=headers, timeout=10)
    data = r.json()
    if data.get("code") != 0:
        raise RuntimeError(data.get("message", "æ¥å£è¿”å›å¼‚å¸¸"))

    d = data["data"]
    stat = d.get("stat", {})
    owner = d.get("owner", {})

    return {
        "bvid": bvid,
        "title": d.get("title"),
        "pubdate": pd.to_datetime(d.get("pubdate", 0), unit="s", errors="coerce"),
        "owner_mid": owner.get("mid"),
        "owner_name": owner.get("name"),
        "view": stat.get("view", 0),
        "like": stat.get("like", 0),
        "coin": stat.get("coin", 0),
        "favorite": stat.get("favorite", 0),
        "reply": stat.get("reply", 0),
        "danmaku": stat.get("danmaku", 0),
        "share": stat.get("share", 0),
        "fans_delta": 0,  # é»˜è®¤0ï¼›ä½ åç»­å¯ç”¨CSV/å¿«ç…§è¡¥é½
        "fetched_at": pd.Timestamp.now(),
    }

# =========================
# Session State
# =========================
if "rows" not in st.session_state:
    st.session_state["rows"] = []  # list[dict]

def append_df_to_session(df_new: pd.DataFrame):
    """Append normalized df rows into session_state rows; de-duplicate by (project,bvid)."""
    if df_new is None or df_new.empty:
        return
    existing = pd.DataFrame(st.session_state["rows"])
    if existing.empty:
        st.session_state["rows"] = df_new.to_dict("records")
        return

    existing = normalize_df(existing)
    df_new = normalize_df(df_new)

    merged = pd.concat([existing, df_new], ignore_index=True)
    # å»é‡ï¼šåŒä¸€é¡¹ç›®åŒä¸€BVï¼Œä¿ç•™æœ€æ–° fetched_at
    merged = merged.sort_values("fetched_at", ascending=True)
    merged = merged.drop_duplicates(subset=["project", "bvid"], keep="last")

    st.session_state["rows"] = merged.to_dict("records")

# =========================
# Sidebar UI
# =========================
st.sidebar.title("ğŸ“Š Bç«™è¿è¥Dashboard")

mode = st.sidebar.radio("æ•°æ®æ¥æº", ["ç²˜è´´é“¾æ¥/BVé‡‡é›†", "ä¸Šä¼ CSVå¯¼å…¥"], index=0)

# ---- CSV æ¨¡æ¿ä¸‹è½½ï¼ˆå¯é€‰ï¼‰
st.sidebar.markdown("#### CSVæ¨¡æ¿ï¼ˆå¯é€‰ï¼‰")
template_df = pd.DataFrame([{
    "project": "æ•åˆ€æ­Œ",
    "bvid": "BVxxxxxxxxxxx",
    "url": "https://www.bilibili.com/video/BVxxxxxxxxxxx",
    "title": "ç¤ºä¾‹æ ‡é¢˜",
    "owner_name": "ç¤ºä¾‹UPä¸»",
    "pubdate": "2026-02-01",
    "view": 1566000,
    "like": 52000,
    "coin": 12000,
    "favorite": 18000,
    "reply": 8000,
    "danmaku": 5000,
    "share": 1200,
    "fans_delta": 3200,
}])
csv_bytes = template_df.to_csv(index=False).encode("utf-8-sig")
st.sidebar.download_button("ä¸‹è½½CSVæ¨¡æ¿", data=csv_bytes, file_name="bili_dashboard_template.csv", mime="text/csv")

st.sidebar.divider()

# ---- Mode A: Link/BV collection
if mode == "ç²˜è´´é“¾æ¥/BVé‡‡é›†":
    project = st.sidebar.text_input("é¡¹ç›®åï¼ˆç”¨äºå½’æ¡£ï¼‰", value="æœªå‘½åé¡¹ç›®")
    links = st.sidebar.text_area("ç²˜è´´è§†é¢‘é“¾æ¥/ BVå·ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰")
    add_btn = st.sidebar.button("â• é‡‡é›†å¹¶å…¥åº“")

    if add_btn:
        items = [x for x in links.splitlines() if x.strip()]
        ok, fail = 0, 0
        rows = []

        for it in items:
            bvid = parse_bvid(it)
            if not bvid:
                fail += 1
                continue
            try:
                row = fetch_video_stats_by_bvid(bvid)
                row["project"] = project
                row["url"] = it
                rows.append(row)
                ok += 1
                time.sleep(0.4)  # æ”¾æ…¢ï¼Œé™ä½è¢«é™æµæ¦‚ç‡
            except Exception:
                fail += 1

        if rows:
            append_df_to_session(pd.DataFrame(rows))
        st.sidebar.success(f"æˆåŠŸé‡‡é›† {ok} æ¡ï¼Œå¤±è´¥ {fail} æ¡ï¼ˆå¤±è´¥å¯ç”¨CSVå¯¼å…¥å…œåº•ï¼‰")

# ---- Mode B: CSV import
else:
    st.sidebar.markdown("#### ä¸Šä¼ CSVå¹¶è‡ªåŠ¨å½’æ¡£")
    default_project = st.sidebar.text_input("ç¼ºå°‘ project åˆ—æ—¶ï¼šé»˜è®¤é¡¹ç›®å", value="æœªå‘½åé¡¹ç›®")
    uploaded = st.sidebar.file_uploader("é€‰æ‹©CSVæ–‡ä»¶", type=["csv"])

    import_btn = st.sidebar.button("ğŸ“¥ å¯¼å…¥CSVåˆ°ä»ªè¡¨ç›˜")

    if import_btn:
        if not uploaded:
            st.sidebar.error("è¯·å…ˆé€‰æ‹©ä¸€ä¸ªCSVæ–‡ä»¶ã€‚")
        else:
            raw = uploaded.getvalue()
            # å…¼å®¹ä¸­æ–‡å¸¸è§ç¼–ç ï¼šutf-8-sig / gbk
            df_csv = None
            for enc in ["utf-8-sig", "utf-8", "gbk"]:
                try:
                    df_csv = pd.read_csv(io.BytesIO(raw), encoding=enc)
                    break
                except Exception:
                    df_csv = None

            if df_csv is None:
                st.sidebar.error("CSVè¯»å–å¤±è´¥ï¼šè¯·ç¡®è®¤æ–‡ä»¶ç¼–ç ï¼ˆå»ºè®®ç”¨UTF-8ï¼‰æˆ–æ ¼å¼æ­£ç¡®ã€‚")
            else:
                df_csv = normalize_df(df_csv)

                # å¦‚æœå¯¼å…¥å project å…¨ä¸ºç©ºï¼Œç”¨é»˜è®¤é¡¹ç›®åè¡¥é½
                if "project" not in df_csv.columns:
                    df_csv["project"] = default_project
                df_csv["project"] = df_csv["project"].apply(lambda x: _safe_str(x).strip())
                df_csv.loc[df_csv["project"] == "", "project"] = default_project

                append_df_to_session(df_csv)
                st.sidebar.success(f"å¯¼å…¥æˆåŠŸï¼š{len(df_csv):,} è¡Œï¼ˆå·²æŒ‰ project å½’æ¡£/å¯ç­›é€‰ï¼‰")

# =========================
# Main Data
# =========================
df = pd.DataFrame(st.session_state["rows"])
df = normalize_df(df) if not df.empty else df

# Top bar controls
st.title("Bç«™æ—¥å¸¸è¿è¥æ•°æ® Dashboard")

if df.empty:
    st.info("å·¦ä¾§é€‰æ‹©æ•°æ®æ¥æºï¼šç²˜è´´BV/é“¾æ¥é‡‡é›† æˆ– ä¸Šä¼ CSVå¯¼å…¥ã€‚")
    st.stop()

df = compute_metrics(df)

# Filters
st.sidebar.divider()
projects = sorted([p for p in df["project"].dropna().unique().tolist() if str(p).strip() != ""])
sel_projects = st.sidebar.multiselect("é€‰æ‹©é¡¹ç›®ï¼ˆç­›é€‰å±•ç¤ºï¼‰", projects, default=projects if projects else None)
df_f = df[df["project"].isin(sel_projects)].copy() if sel_projects else df.copy()

# =========================
# KPI cards
# =========================
c1, c2, c3, c4 = st.columns(4)
c1.metric("æ€»æ’­æ”¾", f"{int(df_f['view'].sum()):,}")
c2.metric("æ€»äº’åŠ¨(èµ+å¸+è—+è¯„)", f"{int(df_f['engagement'].sum()):,}")
c3.metric("å¹³å‡äº’åŠ¨ç‡", f"{df_f['engagement_rate'].mean()*100:.2f}%")
c4.metric("æ·±åº¦ä¿¡å·å æ¯”(å¸+è—/äº’åŠ¨)", f"{df_f['deep_signal_ratio'].mean()*100:.1f}%")

# =========================
# Table
# =========================
st.subheader("é¡¹ç›®å†…è§†é¢‘è¡¨ç°ï¼ˆæŒ‰æ’­æ”¾æ’åºï¼‰")
show_cols = [
    "project", "bvid", "title", "owner_name", "pubdate",
    "view", "like", "coin", "favorite", "reply", "danmaku", "share", "fans_delta",
    "engagement_rate", "deep_signal_ratio"
]
existing_cols = [c for c in show_cols if c in df_f.columns]
st.dataframe(
    df_f[existing_cols].sort_values("view", ascending=False),
    use_container_width=True,
    height=340,
)

# =========================
# Top/Bottom per project + UP baseline compare
# =========================
st.subheader("Top / Bottom æ·±æŒ–ï¼ˆå«Upä¸»åŸºå‡†å¯¹æ¯”ï¼‰")
for proj in (sel_projects if sel_projects else projects):
    d = df_f[df_f["project"] == proj].sort_values("view", ascending=False)
    if d.empty:
        continue

    top = d.iloc[0]
    bottom = d.iloc[-1]

    st.markdown(f"### é¡¹ç›®ï¼š{proj}")
    left, right = st.columns(2)

    def render_card(col, row, tag):
        up = row.get("owner_name", "")
        base = df[df["owner_name"] == up] if up else df

        mean_v, std_v = base["view"].mean(), base["view"].std(ddof=0)
        mean_er, std_er = base["engagement_rate"].mean(), base["engagement_rate"].std(ddof=0)

        col.markdown(f"**{tag}ï¼š{row.get('title','')}**")
        col.caption(f"UPï¼š{up} ï½œ BVï¼š{row.get('bvid','')} ï½œ å‘å¸ƒï¼š{row.get('pubdate','')}")
        col.metric("æ’­æ”¾", f"{int(row.get('view',0)):,}", label_vs_baseline(float(row.get("view",0)), mean_v, std_v))
        col.metric(
            "äº’åŠ¨ç‡",
            f"{float(row.get('engagement_rate',0))*100:.2f}%",
            label_vs_baseline(float(row.get("engagement_rate",0)), mean_er, std_er),
        )

        like = int(row.get("like", 0))
        coin = int(row.get("coin", 0))
        fav = int(row.get("favorite", 0))
        rep = int(row.get("reply", 0))
        deep = float(row.get("deep_signal_ratio", 0))*100

        col.write(f"- èµ/å¸/è—/è¯„ï¼š{like:,}/{coin:,}/{fav:,}/{rep:,}")
        col.write(f"- æ·±åº¦ä¿¡å·å æ¯”ï¼š{deep:.1f}%")

        # ç²‰ä¸å¢é•¿ï¼ˆå¦‚æœæœ‰ï¼‰
        if "fans_delta" in df.columns:
            fd = int(row.get("fans_delta", 0))
            col.write(f"- ç²‰ä¸å‡€å¢ï¼ˆå¦‚æœ‰ï¼‰ï¼š{fd:,}")

    render_card(left, top, "ğŸ”¥ æœ€é«˜æ’­æ”¾")
    render_card(right, bottom, "ğŸ§Š æœ€ä½æ’­æ”¾")

# =========================
# Distribution chart
# =========================
st.subheader("äº’åŠ¨ç‡åˆ†å¸ƒï¼ˆé¡¹ç›®/UPä¸»å¿«é€Ÿå®šä½å¼‚å¸¸ï¼‰")
fig = px.box(
    df_f,
    x="project",
    y="engagement_rate",
    points="all",
    hover_data=[c for c in ["title", "owner_name", "view"] if c in df_f.columns]
)
st.plotly_chart(fig, use_container_width=True)

# =========================
# Auto insights
# =========================
st.subheader("è‡ªåŠ¨è§£è¯»ï¼ˆå¯å¤åˆ¶è¿›å‘¨æŠ¥ï¼‰")
best = df_f.sort_values("view", ascending=False).iloc[0]
worst = df_f.sort_values("view", ascending=True).iloc[0]

insights = []
insights.append(
    f"1ï¼‰æœ¬æœŸæœ€é«˜æ’­æ”¾æ¥è‡ªã€Š{best['title']}ã€‹ï¼ˆ{int(best['view']):,} æ’­æ”¾ï¼‰ï¼Œäº’åŠ¨ç‡ {best['engagement_rate']*100:.2f}%ï¼Œæ·±åº¦ä¿¡å·å æ¯” {best['deep_signal_ratio']*100:.1f}%ã€‚"
)
insights.append(
    f"2ï¼‰æœ€ä½æ’­æ”¾ä¸ºã€Š{worst['title']}ã€‹ï¼ˆ{int(worst['view']):,} æ’­æ”¾ï¼‰ï¼Œäº’åŠ¨ç‡ {worst['engagement_rate']*100:.2f}%ã€‚å»ºè®®ä¼˜å…ˆæ£€æŸ¥ï¼šå°é¢/æ ‡é¢˜ä¿¡æ¯å¯†åº¦ã€å‘å¸ƒæ—¶é—´æ®µã€ä»¥åŠè¯„è®ºåŒºç½®é¡¶å¼•å¯¼ï¼ˆæé—®/æŠ•ç¥¨/ç¦åˆ©ç‚¹ï¼‰ã€‚"
)

deep_mean = df_f["deep_signal_ratio"].mean()
if deep_mean < 0.35:
    insights.append(
        "3ï¼‰æ•´ä½“æ·±åº¦ä¿¡å·åä½ï¼ˆå¸+è—åœ¨äº’åŠ¨ä¸­çš„å æ¯”ä¸é«˜ï¼‰ï¼Œè¯´æ˜å†…å®¹æ›´å¤šæ˜¯â€œè·¯è¿‡å‹çƒ­åº¦â€ã€‚å»ºè®®ï¼šä»·å€¼ç‚¹å‰ç½®ã€ç»“å°¾å¼ºåŒ–æ”¶è—/æŠ•å¸ç†ç”±ã€åšç³»åˆ—åŒ–æ‰¿è¯ºï¼ˆä¸‹ä¸€æœŸçœ‹ç‚¹ï¼‰ã€‚"
    )
else:
    insights.append(
        "3ï¼‰æ•´ä½“æ·±åº¦ä¿¡å·å¥åº·ï¼ˆå¸+è—å æ¯”é«˜ï¼‰ï¼Œè¯´æ˜å†…å®¹å…·å¤‡æ²‰æ·€å±æ€§ã€‚å»ºè®®å›´ç»•è¯¥æ–¹å‘åšç³»åˆ—åŒ–ä¸å›ºå®šæ ç›®èŠ‚å¥ï¼Œæå‡å¯é¢„æœŸçš„å¤çœ‹ä¸å…³æ³¨è½¬åŒ–ã€‚"
    )

# ç²‰ä¸å‡€å¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
if "fans_delta" in df_f.columns and df_f["fans_delta"].abs().sum() > 0:
    total_fd = int(df_f["fans_delta"].sum())
    insights.append(f"4ï¼‰é¡¹ç›®å£å¾„ä¸‹ç²‰ä¸å‡€å¢åˆè®¡ï¼š{total_fd:,}ï¼ˆå¦‚è¯¥åˆ—æ¥è‡ªCSV/å¿«ç…§å£å¾„ï¼Œå¯ä½œä¸ºè½¬ç²‰æ•ˆç‡å¤ç›˜ä¾æ®ï¼‰ã€‚")

st.write("\n".join(insights))
