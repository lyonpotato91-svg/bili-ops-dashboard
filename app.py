import re
import time
import io
import requests
import numpy as np
import pandas as pd
import streamlit as st
import plotly.express as px

st.set_page_config(page_title="Bç«™è¿è¥æ•°æ®Dashboard", layout="wide")

# =========================
# Utils
# =========================
NUM_COLS = ["view", "like", "coin", "favorite", "reply", "danmaku", "share", "fans_delta"]
EXTRA_COLS = ["baseline_for", "data_type"]  # ç»™KOLæ¨¡å—ç”¨ï¼šæ ‡è®°åŸºå‡†å½’å±/åˆä½œoråŸºå‡†

def parse_bvid(url_or_bv: str) -> str | None:
    s = (url_or_bv or "").strip()
    m = re.search(r"(BV[0-9A-Za-z]{10})", s)
    return m.group(1) if m else None

def _safe_int(x, default=0):
    try:
        if pd.isna(x):
            return default
        if isinstance(x, str):
            x = x.replace(",", "").strip()
        return int(float(x))
    except Exception:
        return default

def _safe_str(x, default=""):
    try:
        if pd.isna(x):
            return default
        return str(x)
    except Exception:
        return default

def _safe_date(x):
    try:
        if pd.isna(x):
            return pd.NaT
        return pd.to_datetime(x, errors="coerce")
    except Exception:
        return pd.NaT

def normalize_df(df: pd.DataFrame) -> pd.DataFrame:
    """Standardize columns + dtypes, and ensure required columns exist."""
    df = df.copy()

    df.columns = [str(c).strip() for c in df.columns]

    # Try to map common Chinese headers to standard schema
    zh_alias = {
        "é¡¹ç›®": "project",
        "é¡¹ç›®å": "project",
        "è§†é¢‘é“¾æ¥": "url",
        "é“¾æ¥": "url",
        "æ ‡é¢˜": "title",
        "UPä¸»": "owner_name",
        "UPä¸»åç§°": "owner_name",
        "å‘å¸ƒæ—¶é—´": "pubdate",
        "æ’­æ”¾": "view",
        "æ’­æ”¾é‡": "view",
        "ç‚¹èµ": "like",
        "æŠ•å¸": "coin",
        "æ”¶è—": "favorite",
        "è¯„è®º": "reply",
        "å¼¹å¹•": "danmaku",
        "åˆ†äº«": "share",
        "ç²‰ä¸å¢é•¿": "fans_delta",
        "ç²‰ä¸å¢é‡": "fans_delta",
        "BV": "bvid",
        "BVå·": "bvid",
        "bvid": "bvid",
        "åŸºå‡†å½’å±": "baseline_for",
        "æ•°æ®ç±»å‹": "data_type",
    }

    rename = {}
    for c in df.columns:
        key = str(c).strip()
        if key in zh_alias:
            rename[c] = zh_alias[key]
        else:
            low = key.lower()
            if low in ["project","url","bvid","title","owner_name","owner_mid","pubdate",
                       "view","like","coin","favorite","reply","danmaku","share","fans_delta","fetched_at",
                       "baseline_for","data_type"]:
                rename[c] = low

    df = df.rename(columns=rename)

    # If url exists but bvid missing, try parse
    if "bvid" not in df.columns and "url" in df.columns:
        df["bvid"] = df["url"].apply(parse_bvid)

    # If bvid exists but has URLs inside, parse them
    if "bvid" in df.columns:
        df["bvid"] = df["bvid"].apply(lambda x: parse_bvid(x) if isinstance(x, str) else x)
        df["bvid"] = df["bvid"].apply(lambda x: _safe_str(x))

    # Ensure required text cols exist
    for col in ["project", "title", "owner_name"]:
        if col not in df.columns:
            df[col] = ""

    for col in EXTRA_COLS:
        if col not in df.columns:
            df[col] = ""

    # Parse dates
    if "pubdate" not in df.columns:
        df["pubdate"] = pd.NaT
    df["pubdate"] = df["pubdate"].apply(_safe_date)

    # Ensure numeric cols exist
    for col in NUM_COLS:
        if col not in df.columns:
            df[col] = 0
        df[col] = df[col].apply(_safe_int)

    # Ensure fetched_at exists
    if "fetched_at" not in df.columns:
        df["fetched_at"] = pd.Timestamp.now()
    else:
        df["fetched_at"] = pd.to_datetime(df["fetched_at"], errors="coerce")
        df["fetched_at"] = df["fetched_at"].fillna(pd.Timestamp.now())

    # Keep only known cols
    keep = set(["project","bvid","url","title","pubdate","owner_mid","owner_name",
                "view","like","coin","favorite","reply","danmaku","share","fans_delta","fetched_at",
                "baseline_for","data_type"])
    existing = [c for c in df.columns if c in keep]
    df = df[existing].copy()

    # Drop rows without BV
    if "bvid" in df.columns:
        df = df[df["bvid"].astype(str).str.startswith("BV")]

    return df

def compute_metrics(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df["engagement"] = df["like"] + df["coin"] + df["favorite"] + df["reply"]
    df["engagement_rate"] = np.where(df["view"] > 0, df["engagement"] / df["view"], 0.0)
    df["coin_rate"] = np.where(df["view"] > 0, df["coin"] / df["view"], 0.0)
    df["fav_rate"] = np.where(df["view"] > 0, df["favorite"] / df["view"], 0.0)
    df["reply_rate"] = np.where(df["view"] > 0, df["reply"] / df["view"], 0.0)
    df["deep_signal_ratio"] = np.where(
        df["engagement"] > 0, (df["coin"] + df["favorite"]) / df["engagement"], 0.0
    )
    return df

def label_vs_baseline(value: float, baseline_mean: float, baseline_std: float) -> str:
    if baseline_std <= 1e-9 or np.isnan(baseline_std):
        return "æ­£å¸¸å‘æŒ¥"
    z = (value - baseline_mean) / baseline_std
    if z >= 1.0:
        return "è¶…å¸¸å‘æŒ¥"
    if z <= -1.0:
        return "ä½äºé¢„æœŸ"
    return "æ­£å¸¸å‘æŒ¥"

# =========================
# Bç«™æŠ“å–ï¼ˆé“¾æ¥/BVé‡‡é›†ï¼‰
# =========================
def fetch_video_stats_by_bvid(bvid: str) -> dict:
    """
    è¿”å›å­—æ®µï¼štitle, pubdate, owner_mid, owner_name, view, like, coin, favorite, reply, danmaku, share
    æ³¨æ„ï¼šæ¥å£å¯å˜ï¼›å¤±è´¥ä¼šæŠ›é”™ï¼Œå‰ç«¯æç¤ºæ”¹ç”¨CSVå¯¼å…¥å…œåº•ã€‚
    """
    api = "https://api.bilibili.com/x/web-interface/view"
    headers = {"User-Agent": "Mozilla/5.0"}  # âœ… æå‡ç¨³å®šæ€§
    r = requests.get(api, params={"bvid": bvid}, headers=headers, timeout=10)
    data = r.json()
    if data.get("code") != 0:
        raise RuntimeError(data.get("message", "æ¥å£è¿”å›å¼‚å¸¸"))

    d = data["data"]
    stat = d.get("stat", {})
    owner = d.get("owner", {})
    return {
        "bvid": bvid,
        "title": d.get("title"),
        "pubdate": pd.to_datetime(d.get("pubdate", 0), unit="s", errors="coerce"),
        "owner_mid": owner.get("mid"),
        "owner_name": owner.get("name"),
        "view": stat.get("view", 0),
        "like": stat.get("like", 0),
        "coin": stat.get("coin", 0),
        "favorite": stat.get("favorite", 0),
        "reply": stat.get("reply", 0),
        "danmaku": stat.get("danmaku", 0),
        "share": stat.get("share", 0),
        "fans_delta": 0,  # é»˜è®¤0ï¼›å¯ç”¨CSV/å¿«ç…§è¡¥é½
        "fetched_at": pd.Timestamp.now(),
    }

# KOLæ¨¡å—ï¼šæŠ“UPæœ€è¿‘Næ¡è§†é¢‘BVï¼ˆç”¨äºè¡¥é½æ—¥å¸¸å¯¹æ¯”æ ·æœ¬ï¼‰
def fetch_recent_bvids_by_mid(mid: int, n: int = 5) -> list[dict]:
    api = "https://api.bilibili.com/x/space/arc/search"
    headers = {"User-Agent": "Mozilla/5.0"}
    r = requests.get(api, params={"mid": mid, "pn": 1, "ps": n, "order": "pubdate"}, headers=headers, timeout=10)
    j = r.json()
    if j.get("code") != 0:
        raise RuntimeError(j.get("message", "UPå…¬å¼€è§†é¢‘åˆ—è¡¨æ¥å£å¼‚å¸¸"))
    vlist = (((j.get("data") or {}).get("list") or {}).get("vlist")) or []
    out = []
    for v in vlist:
        bvid = v.get("bvid")
        if not bvid:
            continue
        out.append({
            "bvid": bvid,
            "title": v.get("title", ""),
            "pubdate": pd.to_datetime(v.get("created", 0), unit="s", errors="coerce"),
        })
    return out

# =========================
# çŠ¶æ€å­˜å‚¨ï¼ˆç®€åŒ–ï¼šç”¨ sessionï¼›ä½ ä¸Šçº¿å¯æ¢ SQLiteï¼‰
# =========================
if "rows" not in st.session_state:
    st.session_state["rows"] = []

def append_df_to_session(df_new: pd.DataFrame):
    """Append normalized df rows; de-duplicate by (project,bvid)."""
    if df_new is None or df_new.empty:
        return
    existing = pd.DataFrame(st.session_state["rows"])
    if existing.empty:
        st.session_state["rows"] = normalize_df(df_new).to_dict("records")
        return

    existing = normalize_df(existing)
    df_new = normalize_df(df_new)

    merged = pd.concat([existing, df_new], ignore_index=True)
    merged = merged.sort_values("fetched_at", ascending=True)
    merged = merged.drop_duplicates(subset=["project", "bvid"], keep="last")
    st.session_state["rows"] = merged.to_dict("records")

# =========================
# Sidebar UI
# =========================
st.sidebar.title("ğŸ“Š Bç«™è¿è¥Dashboard")
mode = st.sidebar.radio("æ•°æ®æ¥æº", ["ç²˜è´´é“¾æ¥/BVé‡‡é›†", "ä¸Šä¼ CSVå¯¼å…¥"], index=0)

# CSVæ¨¡æ¿
st.sidebar.markdown("#### CSVæ¨¡æ¿ï¼ˆå¯é€‰ï¼‰")
template_df = pd.DataFrame([{
    "project": "æ•åˆ€æ­Œ",
    "bvid": "BVxxxxxxxxxxx",
    "url": "https://www.bilibili.com/video/BVxxxxxxxxxxx",
    "title": "ç¤ºä¾‹æ ‡é¢˜",
    "owner_name": "ç¤ºä¾‹UPä¸»",
    "pubdate": "2026-02-01",
    "view": 1566000,
    "like": 52000,
    "coin": 12000,
    "favorite": 18000,
    "reply": 8000,
    "danmaku": 5000,
    "share": 1200,
    "fans_delta": 3200,
    "baseline_for": "",
    "data_type": "collab",
}])
csv_bytes = template_df.to_csv(index=False).encode("utf-8-sig")
st.sidebar.download_button("ä¸‹è½½CSVæ¨¡æ¿", data=csv_bytes, file_name="bili_dashboard_template.csv", mime="text/csv")
st.sidebar.divider()

if mode == "ç²˜è´´é“¾æ¥/BVé‡‡é›†":
    project = st.sidebar.text_input("é¡¹ç›®åï¼ˆç”¨äºå½’æ¡£ï¼‰", value="æœªå‘½åé¡¹ç›®")
    links = st.sidebar.text_area("ç²˜è´´è§†é¢‘é“¾æ¥/ BVå·ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰")
    add_btn = st.sidebar.button("â• é‡‡é›†å¹¶å…¥åº“")

    if add_btn:
        items = [x for x in links.splitlines() if x.strip()]
        ok, fail = 0, 0
        rows = []
        for it in items:
            bvid = parse_bvid(it)
            if not bvid:
                fail += 1
                continue
            try:
                row = fetch_video_stats_by_bvid(bvid)
                row["project"] = project
                row["url"] = it
                if not row.get("data_type"):
                    row["data_type"] = "collab"
                rows.append(row)
                ok += 1
                time.sleep(0.4)  # é˜²é™æµ
            except Exception:
                fail += 1
        if rows:
            append_df_to_session(pd.DataFrame(rows))
        st.sidebar.success(f"æˆåŠŸé‡‡é›† {ok} æ¡ï¼Œå¤±è´¥ {fail} æ¡ï¼ˆå¤±è´¥å¯ç”¨CSVå¯¼å…¥å…œåº•ï¼‰")
else:
    st.sidebar.markdown("#### ä¸Šä¼ CSVå¹¶è‡ªåŠ¨å½’æ¡£")
    default_project = st.sidebar.text_input("ç¼ºå°‘ project åˆ—æ—¶ï¼šé»˜è®¤é¡¹ç›®å", value="æœªå‘½åé¡¹ç›®")
    uploaded = st.sidebar.file_uploader("é€‰æ‹©CSVæ–‡ä»¶", type=["csv"])
    import_btn = st.sidebar.button("ğŸ“¥ å¯¼å…¥CSVåˆ°ä»ªè¡¨ç›˜")

    if import_btn:
        if not uploaded:
            st.sidebar.error("è¯·å…ˆé€‰æ‹©ä¸€ä¸ªCSVæ–‡ä»¶ã€‚")
        else:
            raw = uploaded.getvalue()
            df_csv = None
            for enc in ["utf-8-sig", "utf-8", "gbk"]:
                try:
                    df_csv = pd.read_csv(io.BytesIO(raw), encoding=enc)
                    break
                except Exception:
                    df_csv = None
            if df_csv is None:
                st.sidebar.error("CSVè¯»å–å¤±è´¥ï¼šè¯·ç¡®è®¤æ–‡ä»¶ç¼–ç ï¼ˆå»ºè®®ç”¨UTF-8ï¼‰æˆ–æ ¼å¼æ­£ç¡®ã€‚")
            else:
                df_csv = normalize_df(df_csv)
                if "project" not in df_csv.columns:
                    df_csv["project"] = default_project
                df_csv["project"] = df_csv["project"].apply(lambda x: _safe_str(x).strip())
                df_csv.loc[df_csv["project"] == "", "project"] = default_project

                # æ²¡å†™data_typeé»˜è®¤å½“ä½œåˆä½œæ•°æ®ï¼ˆä½ ä¹Ÿå¯ä»¥åœ¨CSVåŒºåˆ†ï¼‰
                if "data_type" not in df_csv.columns:
                    df_csv["data_type"] = "collab"

                append_df_to_session(df_csv)
                st.sidebar.success(f"å¯¼å…¥æˆåŠŸï¼š{len(df_csv):,} è¡Œï¼ˆå·²æŒ‰ project å½’æ¡£/å¯ç­›é€‰ï¼‰")

# =========================
# Main Data
# =========================
df = pd.DataFrame(st.session_state["rows"])
df = normalize_df(df) if not df.empty else df

st.title("Bç«™æ—¥å¸¸è¿è¥æ•°æ® Dashboard")
if df.empty:
    st.info("å·¦ä¾§é€‰æ‹©æ•°æ®æ¥æºï¼šç²˜è´´BV/é“¾æ¥é‡‡é›† æˆ– ä¸Šä¼ CSVå¯¼å…¥ã€‚")
    st.stop()

df = compute_metrics(df)

# Filters
st.sidebar.divider()
projects = sorted([p for p in df["project"].dropna().unique().tolist() if str(p).strip() != ""])
sel_projects = st.sidebar.multiselect("é€‰æ‹©é¡¹ç›®ï¼ˆç­›é€‰å±•ç¤ºï¼‰", projects, default=projects if projects else None)
df_f = df[df["project"].isin(sel_projects)].copy() if sel_projects else df.copy()

# =========================
# KPI cardsï¼ˆä¿æŒä¸å˜ï¼‰
# =========================
c1, c2, c3, c4 = st.columns(4)
c1.metric("æ€»æ’­æ”¾", f"{int(df_f['view'].sum()):,}")
c2.metric("æ€»äº’åŠ¨(èµ+å¸+è—+è¯„)", f"{int(df_f['engagement'].sum()):,}")
c3.metric("å¹³å‡äº’åŠ¨ç‡", f"{df_f['engagement_rate'].mean()*100:.2f}%")
c4.metric("æ·±åº¦ä¿¡å·å æ¯”(å¸+è—/äº’åŠ¨)", f"{df_f['deep_signal_ratio'].mean()*100:.1f}%")

# =========================================================
# è·¨é¡¹ç›®å¯¹æ¯”ï¼ˆä¿æŒä¸å˜ï¼šæ’è¡Œæ¦œ + å››è±¡é™ + é¡¹ç›®è§£è¯»ï¼‰
# =========================================================
st.subheader("è·¨é¡¹ç›®å¯¹æ¯”ï¼ˆé¡¹ç›®ä¹‹é—´è°æ›´å¼ºã€è°æ›´ç¨³ï¼‰")

proj_rows = []
for proj, g in df_f.groupby("project"):
    g2 = g.sort_values("view", ascending=False).copy()
    total_view = int(g2["view"].sum())
    total_eng = int(g2["engagement"].sum())
    video_cnt = int(len(g2))
    up_cnt = int(g2["owner_name"].nunique()) if "owner_name" in g2.columns else 0

    er_med = float(g2["engagement_rate"].median())
    deep_med = float(g2["deep_signal_ratio"].median())

    er_q1 = float(g2["engagement_rate"].quantile(0.25))
    er_q3 = float(g2["engagement_rate"].quantile(0.75))
    er_iqr = er_q3 - er_q1

    top1_view = int(g2.iloc[0]["view"]) if video_cnt > 0 else 0
    top3_view = int(g2.head(3)["view"].sum()) if video_cnt > 0 else 0
    top1_share = (top1_view / total_view) if total_view > 0 else 0.0
    top3_share = (top3_view / total_view) if total_view > 0 else 0.0

    proj_rows.append({
        "project": proj,
        "è§†é¢‘æ•°": video_cnt,
        "UPæ•°": up_cnt,
        "æ€»æ’­æ”¾": total_view,
        "æ€»äº’åŠ¨": total_eng,
        "äº’åŠ¨ç‡ä¸­ä½æ•°": er_med,
        "æ·±åº¦ä¿¡å·ä¸­ä½æ•°": deep_med,
        "äº’åŠ¨ç‡æ³¢åŠ¨(IQR)": er_iqr,
        "Top1æ’­æ”¾è´¡çŒ®": top1_share,
        "Top3æ’­æ”¾è´¡çŒ®": top3_share,
    })

proj_df = pd.DataFrame(proj_rows).sort_values("æ€»æ’­æ”¾", ascending=False)

show_proj_cols = ["project","è§†é¢‘æ•°","UPæ•°","æ€»æ’­æ”¾","æ€»äº’åŠ¨","äº’åŠ¨ç‡ä¸­ä½æ•°","æ·±åº¦ä¿¡å·ä¸­ä½æ•°","äº’åŠ¨ç‡æ³¢åŠ¨(IQR)","Top1æ’­æ”¾è´¡çŒ®","Top3æ’­æ”¾è´¡çŒ®"]
st.dataframe(
    proj_df[show_proj_cols]
      .assign(**{
          "äº’åŠ¨ç‡ä¸­ä½æ•°": (proj_df["äº’åŠ¨ç‡ä¸­ä½æ•°"]*100).map(lambda x: f"{x:.2f}%"),
          "æ·±åº¦ä¿¡å·ä¸­ä½æ•°": (proj_df["æ·±åº¦ä¿¡å·ä¸­ä½æ•°"]*100).map(lambda x: f"{x:.1f}%"),
          "äº’åŠ¨ç‡æ³¢åŠ¨(IQR)": (proj_df["äº’åŠ¨ç‡æ³¢åŠ¨(IQR)"]*100).map(lambda x: f"{x:.2f}pp"),
          "Top1æ’­æ”¾è´¡çŒ®": (proj_df["Top1æ’­æ”¾è´¡çŒ®"]*100).map(lambda x: f"{x:.1f}%"),
          "Top3æ’­æ”¾è´¡çŒ®": (proj_df["Top3æ’­æ”¾è´¡çŒ®"]*100).map(lambda x: f"{x:.1f}%"),
      }),
    use_container_width=True,
    height=260
)

st.markdown("**é¡¹ç›®å››è±¡é™ï¼ˆX=äº’åŠ¨ç‡ä¸­ä½æ•°ï¼ŒY=æ·±åº¦ä¿¡å·ä¸­ä½æ•°ï¼‰**")
if len(proj_df) >= 2:
    x_med = float(proj_df["äº’åŠ¨ç‡ä¸­ä½æ•°"].median())
    y_med = float(proj_df["æ·±åº¦ä¿¡å·ä¸­ä½æ•°"].median())

    fig2 = px.scatter(
        proj_df,
        x="äº’åŠ¨ç‡ä¸­ä½æ•°",
        y="æ·±åº¦ä¿¡å·ä¸­ä½æ•°",
        size="æ€»æ’­æ”¾",
        hover_data=["è§†é¢‘æ•°","UPæ•°","æ€»æ’­æ”¾","Top1æ’­æ”¾è´¡çŒ®","Top3æ’­æ”¾è´¡çŒ®","äº’åŠ¨ç‡æ³¢åŠ¨(IQR)"],
        text="project",
    )
    fig2.add_vline(x=x_med, line_dash="dash")
    fig2.add_hline(y=y_med, line_dash="dash")
    fig2.update_traces(textposition="top center")
    fig2.update_layout(xaxis_tickformat=".0%", yaxis_tickformat=".0%")
    st.plotly_chart(fig2, use_container_width=True)

    st.markdown("**é¡¹ç›®è§£è¯»ï¼ˆå¯ç›´æ¥å†™è¿›å‘¨æŠ¥ï¼‰**")
    bullets = []
    for _, r in proj_df.iterrows():
        proj = r["project"]
        er = r["äº’åŠ¨ç‡ä¸­ä½æ•°"]
        deep = r["æ·±åº¦ä¿¡å·ä¸­ä½æ•°"]
        top1 = r["Top1æ’­æ”¾è´¡çŒ®"]
        iqr = r["äº’åŠ¨ç‡æ³¢åŠ¨(IQR)"]

        if er >= x_med and deep >= y_med:
            tag = "åˆçƒ­åˆæ²‰æ·€ï¼ˆä¼˜å…ˆåŠ ç ï¼‰"
        elif er >= x_med and deep < y_med:
            tag = "çƒ­é—¹ä½†åæµ…ï¼ˆå¼ºåŒ–æ”¶è—/æŠ•å¸å¼•å¯¼ï¼‰"
        elif er < x_med and deep >= y_med:
            tag = "å°ä¼—ä½†çœŸçˆ±ï¼ˆé€‚åˆç³»åˆ—åŒ–/ç²¾å‡†æŠ•æ”¾ï¼‰"
        else:
            tag = "åå¼±ï¼ˆé‡ç‚¹å¤ç›˜é€‰é¢˜&åŒ…è£…&åˆ†å‘ï¼‰"

        extra = []
        if top1 >= 0.45:
            extra.append("å¤´éƒ¨ä¾èµ–é«˜ï¼ˆTop1è´¡çŒ®åå¤§ï¼Œéœ€è¡¥ä¸­è…°éƒ¨ï¼‰")
        if len(proj_df) > 0 and iqr >= float(proj_df["äº’åŠ¨ç‡æ³¢åŠ¨(IQR)"].median()):
            extra.append("æ³¢åŠ¨è¾ƒå¤§ï¼ˆå†…å®¹/åˆ†å‘ä¸ç¨³å®šï¼‰")

        extra_txt = f"ï¼›{ 'ï¼›'.join(extra) }" if extra else ""
        bullets.append(
            f"- äº’åŠ¨ç‡ä¸­ä½æ•° {er*100:.2f}%ï¼Œæ·±åº¦ä¿¡å·ä¸­ä½æ•° {deep*100:.1f}%ï¼š{tag}{extra_txt}"
        )
    st.write("\n".join(bullets))
else:
    st.info("é¡¹ç›®æ•°ä¸è¶³ï¼ˆ<2ï¼‰æ—¶ï¼Œå››è±¡é™å¯¹æ¯”æ„ä¹‰ä¸å¤§ã€‚å†å¯¼å…¥/é‡‡é›†è‡³å°‘ä¸¤ä¸ªé¡¹ç›®çš„æ•°æ®å³å¯å±•ç¤ºã€‚")

st.divider()

# =========================================================
# âœ… KOLç‹¬ç«‹æ¨¡å—ï¼ˆæ–°å¢ï¼Œä¸æ”¹åŠ¨å…¶ä»–æ¨¡å—ï¼‰
# - æ ‡æ³¨ï¼šåˆä½œè§†é¢‘æ˜æ˜¾ä¼˜äºæ—¥å¸¸
# - è‡ªåŠ¨è¡¥é½ï¼šæŠ“KOLé¢å¤–4-5æ¡æ—¥å¸¸è§†é¢‘
# - è¾“å‡ºï¼šå•†åŠ¡èµ„æ–™åº“CSVï¼ˆå«ç”»åƒä¸€å¥è¯ & åˆä½œå»ºè®®ç»„åˆï¼‰
# =========================================================
st.subheader("KOLåˆä½œèµ„æ–™åº“ï¼ˆç‹¬ç«‹æ¨¡å—ï¼šæ ‡æ³¨ + æ ‡ç­¾ + ç”»åƒä¸€å¥è¯ + åˆä½œå»ºè®®ç»„åˆ + å¯ä¸‹è½½ï¼‰")

all_projects = sorted([p for p in df["project"].dropna().unique().tolist() if str(p).strip() != ""])
default_collab = sel_projects if sel_projects else all_projects

with st.expander("KOLæ¨¡å—è®¾ç½®ï¼ˆé»˜è®¤å³å¯ï¼‰", expanded=False):
    collab_projects = st.multiselect("å“ªäº›é¡¹ç›®ç®—â€œåˆä½œé¡¹ç›®â€", all_projects, default=default_collab)

    baseline_pref = st.radio(
        "æ—¥å¸¸åŸºå‡†æ€ä¹ˆå–ï¼Ÿ",
        ["ä¼˜å…ˆç”¨éåˆä½œé¡¹ç›®è§†é¢‘ï¼ˆæ›´åƒâ€˜æ—¥å¸¸â€™ï¼‰", "ç”¨è¯¥UPåœ¨åº“é‡Œæ‰€æœ‰è§†é¢‘ï¼ˆæ›´å®½æ¾ï¼‰"],
        index=0
    )
    min_baseline_n = st.slider("åŸºå‡†æœ€å°‘éœ€è¦å¤šå°‘æ¡è§†é¢‘ï¼ˆå¤ªå°‘ä¸åˆ¤å®šï¼‰", 3, 30, 6)

    extra_baseline_n = st.slider("è‡ªåŠ¨è¡¥é½ï¼šæ¯ä¸ªKOLé¢å¤–æŠ“å‡ æ¡æ—¥å¸¸è§†é¢‘", 0, 10, 5)
    sleep_sec = st.slider("æŠ“å–é—´éš”ï¼ˆé˜²é™æµï¼‰", 0.2, 2.0, 0.8, step=0.1)

    lift_view_pct = st.slider("æ’­æ”¾æå‡é˜ˆå€¼ï¼ˆç›¸å¯¹åŸºå‡†ä¸­ä½æ•°ï¼‰", 0, 300, 30, step=5)
    lift_er_pct = st.slider("äº’åŠ¨ç‡æå‡é˜ˆå€¼ï¼ˆç›¸å¯¹åŸºå‡†ä¸­ä½æ•°ï¼‰", 0, 300, 20, step=5)
    lift_deep_pct = st.slider("æ·±åº¦ä¿¡å·æå‡é˜ˆå€¼ï¼ˆç›¸å¯¹åŸºå‡†ä¸­ä½æ•°ï¼‰", 0, 300, 10, step=5)
    z_threshold = st.slider("Zåˆ†æ•°é˜ˆå€¼", 0.0, 3.0, 1.0, step=0.1)
    require_both = st.checkbox("æ›´ä¸¥æ ¼ï¼šæ’­æ”¾&äº’åŠ¨ç‡éƒ½è¦æ˜æ˜¾æ›´å¥½æ‰æ ‡æ³¨", value=False)

btn1, btn2, btn3 = st.columns([1,1,2])
with btn1:
    fetch_baseline_btn = st.button("ğŸ§² è‡ªåŠ¨æŠ“KOLæ—¥å¸¸æ ·æœ¬ï¼ˆè¡¥é½4-5æ¡ï¼‰")
with btn2:
    build_kol_btn = st.button("ğŸ“š ç”Ÿæˆ/åˆ·æ–°KOLå•†åŠ¡èµ„æ–™åº“")
with btn3:
    st.caption("å»ºè®®æµç¨‹ï¼šå…ˆé€‰åˆä½œé¡¹ç›® â†’ï¼ˆå¯é€‰ï¼‰è¡¥é½æ—¥å¸¸æ ·æœ¬ â†’ ç”Ÿæˆèµ„æ–™åº“ â†’ ä¸‹è½½CSVåšåˆä½œæ± ")

if not collab_projects:
    st.info("KOLæ¨¡å—ï¼šè¯·å…ˆé€‰æ‹©è‡³å°‘ä¸€ä¸ªâ€œåˆä½œé¡¹ç›®â€ã€‚")
else:
    df_all = df.copy()
    df_all = df_all[df_all["owner_name"].astype(str).str.strip() != ""].copy()
    collab_df = df_all[df_all["project"].isin(collab_projects)].copy()

    # --- è‡ªåŠ¨è¡¥é½æ—¥å¸¸æ ·æœ¬ ---
    if fetch_baseline_btn:
        if collab_df.empty or extra_baseline_n <= 0:
            st.warning("åˆä½œé¡¹ç›®ä¸‹æ²¡æœ‰æ•°æ®ï¼Œæˆ–è¡¥é½æ•°é‡ä¸º0ã€‚")
        else:
            existed = set(df_all["bvid"].astype(str).tolist())
            rows_new = []

            for up, g in collab_df.groupby("owner_name"):
                mid_vals = g["owner_mid"].dropna().unique().tolist() if "owner_mid" in g.columns else []
                if not mid_vals:
                    continue
                mid = mid_vals[0]

                try:
                    recent = fetch_recent_bvids_by_mid(int(mid), n=int(extra_baseline_n))
                except Exception:
                    continue

                for item in recent:
                    bvid = item["bvid"]
                    if bvid in existed:
                        continue
                    try:
                        row = fetch_video_stats_by_bvid(bvid)
                        row["project"] = f"BASELINE::{up}"  # å•ç‹¬é¡¹ç›®ï¼Œä¸å½±å“ä½ åŸé¡¹ç›®åˆ†æ
                        row["baseline_for"] = up
                        row["data_type"] = "baseline"
                        row["url"] = f"https://www.bilibili.com/video/{bvid}"
                        rows_new.append(row)
                        existed.add(bvid)
                        time.sleep(float(sleep_sec))
                    except Exception:
                        continue

            if rows_new:
                append_df_to_session(pd.DataFrame(rows_new))
                st.success(f"å·²è¡¥é½æ—¥å¸¸æ ·æœ¬ï¼šæ–°å¢ {len(rows_new)} æ¡ï¼ˆé¡¹ç›®åä¸º BASELINE::UPä¸»ï¼‰")
                # é‡æ–°åŠ è½½æ•°æ®
                df = compute_metrics(normalize_df(pd.DataFrame(st.session_state["rows"])))
                df_f = df[df["project"].isin(sel_projects)].copy() if sel_projects else df.copy()
            else:
                st.warning("æœªæŠ“åˆ°å¯æ–°å¢çš„æ—¥å¸¸æ ·æœ¬ï¼ˆå¯èƒ½é™æµ/æ¥å£æ³¢åŠ¨/æ ·æœ¬å·²å­˜åœ¨ï¼‰ã€‚")

    # --- ç”ŸæˆKOLå•†åŠ¡èµ„æ–™åº“ ---
    if build_kol_btn:
        df_all = df.copy()
        df_all = df_all[df_all["owner_name"].astype(str).str.strip() != ""].copy()
        df_all = compute_metrics(df_all)
        collab_df2 = df_all[df_all["project"].isin(collab_projects)].copy()

        if collab_df2.empty:
            st.warning("åˆä½œé¡¹ç›®ä¸‹æ²¡æœ‰æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆèµ„æ–™åº“ã€‚")
        else:
            rows = []
            for up, g_collab in collab_df2.groupby("owner_name"):
                # baseline selection
                if baseline_pref.startswith("ä¼˜å…ˆç”¨éåˆä½œ"):
                    g_base = df_all[(df_all["owner_name"] == up) & (~df_all["project"].isin(collab_projects))].copy()
                    if len(g_base) < min_baseline_n:
                        g_base = df_all[df_all["owner_name"] == up].copy()
                else:
                    g_base = df_all[df_all["owner_name"] == up].copy()

                if len(g_base) < min_baseline_n:
                    continue

                # baseline stats (median + mean/std for z)
                base_view_med = float(g_base["view"].median())
                base_er_med = float(g_base["engagement_rate"].median())
                base_deep_med = float(g_base["deep_signal_ratio"].median())

                base_view_mean = float(g_base["view"].mean())
                base_view_std = float(g_base["view"].std(ddof=0)) if float(g_base["view"].std(ddof=0)) > 1e-9 else 0.0
                base_er_mean = float(g_base["engagement_rate"].mean())
                base_er_std = float(g_base["engagement_rate"].std(ddof=0)) if float(g_base["engagement_rate"].std(ddof=0)) > 1e-12 else 0.0
                base_deep_mean = float(g_base["deep_signal_ratio"].mean())
                base_deep_std = float(g_base["deep_signal_ratio"].std(ddof=0)) if float(g_base["deep_signal_ratio"].std(ddof=0)) > 1e-12 else 0.0

                # collab stats
                collab_view_med = float(g_collab["view"].median())
                collab_er_med = float(g_collab["engagement_rate"].median())
                collab_deep_med = float(g_collab["deep_signal_ratio"].median())

                # lifts
                view_lift = (collab_view_med / base_view_med - 1.0) if base_view_med > 0 else np.nan
                er_lift = (collab_er_med / base_er_med - 1.0) if base_er_med > 0 else np.nan
                deep_lift = (collab_deep_med / base_deep_med - 1.0) if base_deep_med > 0 else np.nan

                # z
                z_view = (collab_view_med - base_view_mean) / base_view_std if base_view_std > 0 else 0.0
                z_er = (collab_er_med - base_er_mean) / base_er_std if base_er_std > 0 else 0.0
                z_deep = (collab_deep_med - base_deep_mean) / base_deep_std if base_deep_std > 0 else 0.0

                cond_view = (not np.isnan(view_lift)) and (view_lift >= lift_view_pct/100.0) and (z_view >= z_threshold)
                cond_er = (not np.isnan(er_lift)) and (er_lift >= lift_er_pct/100.0) and (z_er >= z_threshold)
                cond_deep = (not np.isnan(deep_lift)) and (deep_lift >= lift_deep_pct/100.0) and (z_deep >= z_threshold)

                if require_both:
                    is_good = cond_view and cond_er
                else:
                    is_good = cond_view or cond_er or cond_deep

                # evidence
                top3 = g_collab.sort_values("view", ascending=False).head(3)
                top3_titles = "ï½œ".join([str(t)[:30] for t in top3["title"].tolist()])
                top3_links = "ï½œ".join([f"https://www.bilibili.com/video/{b}" for b in top3["bvid"].tolist()])

                # stability/head dependence inside collab
                collab_sorted = g_collab.sort_values("view", ascending=False)
                total_view = float(collab_sorted["view"].sum())
                top1_share = (float(collab_sorted.iloc[0]["view"]) / total_view) if total_view > 0 else 0.0

                er_q1 = float(g_collab["engagement_rate"].quantile(0.25))
                er_q3 = float(g_collab["engagement_rate"].quantile(0.75))
                er_iqr = er_q3 - er_q1

                # business tags
                tags = []
                if not np.isnan(view_lift) and view_lift >= 0.30:
                    tags.append("çƒ­åº¦æ‹‰å‡å‹")
                if not np.isnan(er_lift) and er_lift >= 0.20:
                    tags.append("å¼ºäº’åŠ¨å¼•çˆ†")
                if not np.isnan(deep_lift) and deep_lift >= 0.10:
                    tags.append("ä»·å€¼æ²‰æ·€å‹")
                if top1_share >= 0.55:
                    tags.append("å¤´éƒ¨ä¾èµ–é«˜")
                if er_iqr >= float(g_collab["engagement_rate"].median()) * 0.8:
                    tags.append("æ³¢åŠ¨è¾ƒå¤§")
                if not tags:
                    tags.append("å¸¸è§„è¡¨ç°")

                # persona one-liner
                heat = "çƒ­åº¦æ‹‰å‡" if (not np.isnan(view_lift) and view_lift >= 0.30) else "çƒ­åº¦ç¨³å®š"
                interact = "äº’åŠ¨å¼º" if (not np.isnan(er_lift) and er_lift >= 0.20) else "äº’åŠ¨å¸¸è§„"
                depth = "æ²‰æ·€å¼º" if (not np.isnan(deep_lift) and deep_lift >= 0.10) else "æ²‰æ·€ä¸€èˆ¬"
                risk = []
                if "å¤´éƒ¨ä¾èµ–é«˜" in tags:
                    risk.append("å¤´éƒ¨ä¾èµ–")
                if "æ³¢åŠ¨è¾ƒå¤§" in tags:
                    risk.append("æ³¢åŠ¨")
                persona = f"{heat} + {interact} + {depth}" + (f"ï¼ˆé£é™©ï¼š{'/'.join(risk)}ï¼‰" if risk else "")

                # suggestion bundle
                if (not np.isnan(view_lift) and view_lift >= 0.30) and (not np.isnan(er_lift) and er_lift >= 0.20):
                    scene = "å¤§ä¿ƒèŠ‚ç‚¹/æ–°å“é¦–å‘/çƒ­ç‚¹å€ŸåŠ¿"
                    form = "é¦–å‘æµ‹è¯„/æŒ‘æˆ˜èµ›/è”åˆä¼åˆ’ï¼ˆå¸¦è¯é¢˜ï¼‰"
                    hook = "å‰3ç§’å¼ºå–ç‚¹ + æ˜ç¡®äº’åŠ¨ä»»åŠ¡ï¼ˆè¯„è®ºæé—®/æŠ•ç¥¨ï¼‰ + ç»“å°¾æŠ•å¸æ”¶è—ç†ç”±"
                    avoid = "é¿å…ç¡¬å¹¿ç›´ç»™ï¼Œå¿…é¡»æ•…äº‹åŒ–/ä½“éªŒåŒ–"
                elif (not np.isnan(deep_lift) and deep_lift >= 0.10):
                    scene = "å£ç¢‘å‘/ç§è‰å‘/é•¿å°¾æŒç»­æ›å…‰"
                    form = "ç³»åˆ—åŒ–æ ç›®/æ·±åº¦æµ‹è¯„/æ¸…å•å‘å†…å®¹"
                    hook = "å¯å¤çœ‹ä»·å€¼ç‚¹ï¼ˆæŠ€å·§/æ”»ç•¥/å¯¹æ¯”ï¼‰+ æ”¶è—å¼•å¯¼ + è¯„è®ºåŒºç½®é¡¶èµ„æ–™"
                    avoid = "åˆ«ç”¨çº¯æ’­æ”¾KPIè€ƒæ ¸ï¼›é‡ç‚¹çœ‹æ”¶è—/æŠ•å¸/æœç´¢é•¿å°¾"
                else:
                    scene = "ä½æˆæœ¬è¯•æ°´/è¡¥ä½æŠ•æ”¾"
                    form = "å•æ¡è½¯æ¤å…¥/ç´ æå…±åˆ›/è¯é¢˜äº’åŠ¨"
                    hook = "å›´ç»•TAæ“…é•¿çš„å†…å®¹ç»“æ„ï¼ˆæ•´æ´»/æµ‹è¯„/ç›˜ç‚¹ï¼‰åšè½»åˆä½œ"
                    avoid = "ä¸è¦é‡æƒç›Šç»‘å®šï¼Œå…ˆç”¨1-2æ¡éªŒè¯å†åŠ ç "

                if "å¤´éƒ¨ä¾èµ–é«˜" in tags:
                    avoid += "ï¼›å»ºè®®å‡†å¤‡ABä¸¤æ¡å¤‡é€‰é€‰é¢˜ï¼Œé™ä½å•æ¡å¤±è´¥é£é™©"
                if "æ³¢åŠ¨è¾ƒå¤§" in tags:
                    avoid += "ï¼›å»ºè®®ç»™æ˜ç¡®briefä¸èµ„æºä½æ”¯æŒï¼Œå‡å°‘å‘æŒ¥æ³¢åŠ¨"

                suggestion_bundle = f"é€‚åˆåœºæ™¯ï¼š{scene}ï½œåˆä½œå½¢å¼ï¼š{form}ï½œå†…å®¹æŠ“æ‰‹ï¼š{hook}ï½œé¿å‘ï¼š{avoid}"

                # short business advice
                if is_good and ("å¤´éƒ¨ä¾èµ–é«˜" not in tags) and ("æ³¢åŠ¨è¾ƒå¤§" not in tags):
                    advice = "ä¼˜å…ˆç»­çº¦/å¯åŠ ç ï¼šåˆä½œå¯¹å…¶è´¦å·è¡¨ç°æœ‰æ˜ç¡®å¢ç›Šï¼Œå¯äº‰å–æ›´æ·±æƒç›Š/ç³»åˆ—åŒ–"
                elif is_good and ("å¤´éƒ¨ä¾èµ–é«˜" in tags or "æ³¢åŠ¨è¾ƒå¤§" in tags):
                    advice = "å¯åˆä½œä½†è¦æ§é£é™©ï¼šå»ºè®®ABé€‰é¢˜+åŠ å¼ºåˆ†å‘èµ„æº+æ˜ç¡®è½¬åŒ–KPI"
                elif (not is_good) and (not np.isnan(deep_lift) and deep_lift >= 0.10):
                    advice = "å°ä¼—é«˜è´¨ï¼šé€‚åˆå‚ç±»/å£ç¢‘åœºæ™¯ï¼Œä¸å»ºè®®ç”¨çº¯æ’­æ”¾KPIè¯„ä¼°"
                else:
                    advice = "è°¨æ…ï¼šå…ˆä½æˆæœ¬è¯•æ°´æˆ–æ¢é€‰é¢˜/åŒ…è£…åå†è¯„ä¼°"

                rows.append({
                    "KOL/UPä¸»": up,
                    "æ ‡æ³¨": "â­ åˆä½œæ˜æ˜¾æ›´å¥½" if is_good else "",
                    "æ ‡ç­¾": "ã€".join(tags),
                    "KOLç”»åƒä¸€å¥è¯": persona,
                    "åˆä½œå»ºè®®ç»„åˆ": suggestion_bundle,
                    "å•†åŠ¡å»ºè®®": advice,

                    "åˆä½œè§†é¢‘æ•°": int(len(g_collab)),
                    "åŸºå‡†è§†é¢‘æ•°": int(len(g_base)),

                    "åˆä½œæ’­æ”¾ä¸­ä½æ•°": collab_view_med,
                    "æ—¥å¸¸æ’­æ”¾ä¸­ä½æ•°": base_view_med,
                    "åˆä½œæ’­æ”¾æå‡": view_lift,

                    "åˆä½œäº’åŠ¨ç‡ä¸­ä½æ•°": collab_er_med,
                    "æ—¥å¸¸äº’åŠ¨ç‡ä¸­ä½æ•°": base_er_med,
                    "åˆä½œäº’åŠ¨ç‡æå‡": er_lift,

                    "åˆä½œæ·±åº¦ä¿¡å·ä¸­ä½æ•°": collab_deep_med,
                    "æ—¥å¸¸æ·±åº¦ä¿¡å·ä¸­ä½æ•°": base_deep_med,
                    "æ·±åº¦ä¿¡å·æå‡": deep_lift,

                    "å¤´éƒ¨ä¾èµ–(Top1è´¡çŒ®)": top1_share,
                    "äº’åŠ¨ç‡æ³¢åŠ¨(IQR)": er_iqr,

                    "è¯æ®-åˆä½œTop3æ ‡é¢˜": top3_titles,
                    "è¯æ®-åˆä½œTop3é“¾æ¥": top3_links,
                })

            if not rows:
                st.warning("KOLèµ„æ–™åº“ç”Ÿæˆå¤±è´¥ï¼šæ—¥å¸¸åŸºå‡†è§†é¢‘ä¸è¶³ã€‚å»ºè®®å…ˆç‚¹â€œè‡ªåŠ¨æŠ“KOLæ—¥å¸¸æ ·æœ¬â€ã€‚")
            else:
                lib = pd.DataFrame(rows)
                lib["_flag"] = lib["æ ‡æ³¨"].apply(lambda x: 1 if str(x).strip() else 0)
                lib = lib.sort_values(["_flag","åˆä½œäº’åŠ¨ç‡æå‡","åˆä½œæ’­æ”¾æå‡"], ascending=[False, False, False]).drop(columns=["_flag"])

                # display (formatted)
                show = lib.copy()
                show["åˆä½œæ’­æ”¾ä¸­ä½æ•°"] = show["åˆä½œæ’­æ”¾ä¸­ä½æ•°"].map(lambda x: f"{int(x):,}")
                show["æ—¥å¸¸æ’­æ”¾ä¸­ä½æ•°"] = show["æ—¥å¸¸æ’­æ”¾ä¸­ä½æ•°"].map(lambda x: f"{int(x):,}")
                show["åˆä½œæ’­æ”¾æå‡"] = show["åˆä½œæ’­æ”¾æå‡"].map(lambda x: "-" if pd.isna(x) else f"{x*100:.1f}%")
                show["åˆä½œäº’åŠ¨ç‡ä¸­ä½æ•°"] = show["åˆä½œäº’åŠ¨ç‡ä¸­ä½æ•°"].map(lambda x: f"{x*100:.2f}%")
                show["æ—¥å¸¸äº’åŠ¨ç‡ä¸­ä½æ•°"] = show["æ—¥å¸¸äº’åŠ¨ç‡ä¸­ä½æ•°"].map(lambda x: f"{x*100:.2f}%")
                show["åˆä½œäº’åŠ¨ç‡æå‡"] = show["åˆä½œäº’åŠ¨ç‡æå‡"].map(lambda x: "-" if pd.isna(x) else f"{x*100:.1f}%")
                show["åˆä½œæ·±åº¦ä¿¡å·ä¸­ä½æ•°"] = show["åˆä½œæ·±åº¦ä¿¡å·ä¸­ä½æ•°"].map(lambda x: f"{x*100:.1f}%")
                show["æ—¥å¸¸æ·±åº¦ä¿¡å·ä¸­ä½æ•°"] = show["æ—¥å¸¸æ·±åº¦ä¿¡å·ä¸­ä½æ•°"].map(lambda x: f"{x*100:.1f}%")
                show["æ·±åº¦ä¿¡å·æå‡"] = show["æ·±åº¦ä¿¡å·æå‡"].map(lambda x: "-" if pd.isna(x) else f"{x*100:.1f}%")
                show["å¤´éƒ¨ä¾èµ–(Top1è´¡çŒ®)"] = show["å¤´éƒ¨ä¾èµ–(Top1è´¡çŒ®)"].map(lambda x: f"{x*100:.1f}%")
                show["äº’åŠ¨ç‡æ³¢åŠ¨(IQR)"] = show["äº’åŠ¨ç‡æ³¢åŠ¨(IQR)"].map(lambda x: f"{x*100:.2f}pp")

                st.dataframe(show, use_container_width=True, height=420)

                out_bytes = lib.to_csv(index=False).encode("utf-8-sig")
                st.download_button("â¬‡ï¸ ä¸‹è½½KOLå•†åŠ¡èµ„æ–™åº“ï¼ˆCSVï¼‰", data=out_bytes, file_name="kol_business_library.csv", mime="text/csv")

                flagged = lib[lib["æ ‡æ³¨"].astype(str).str.contains("â­")]
                if not flagged.empty:
                    st.success(f"å·²æ ‡æ³¨ {len(flagged)} ä½â€œåˆä½œæ˜æ˜¾æ›´å¥½â€çš„KOLï¼ˆå»ºè®®ä¼˜å…ˆä½œä¸ºæœªæ¥åˆä½œæ± ï¼‰ã€‚")
                else:
                    st.warning("å½“å‰é˜ˆå€¼ä¸‹æš‚æ— â€œåˆä½œæ˜æ˜¾æ›´å¥½â€çš„KOLã€‚å¯é€‚å½“é™ä½æå‡é˜ˆå€¼æˆ–Zé˜ˆå€¼ã€‚")

st.divider()

# =========================
# é¡¹ç›®å†…è§†é¢‘è¡¨ç°ï¼ˆä¿æŒä¸å˜ï¼‰
# =========================
st.subheader("é¡¹ç›®å†…è§†é¢‘è¡¨ç°ï¼ˆæŒ‰æ’­æ”¾æ’åºï¼‰")
show_cols = [
    "project","bvid","title","owner_name","pubdate",
    "view","like","coin","favorite","reply","danmaku","share","fans_delta",
    "engagement_rate","deep_signal_ratio"
]
existing_cols = [c for c in show_cols if c in df_f.columns]
st.dataframe(
    df_f[existing_cols].sort_values("view", ascending=False),
    use_container_width=True,
    height=340,
)

# =========================
# Top/Bottom æ·±æŒ–ï¼ˆä¿æŒä¸å˜ï¼‰
# =========================
st.subheader("Top / Bottom æ·±æŒ–ï¼ˆå«Upä¸»åŸºå‡†å¯¹æ¯”ï¼‰")
for proj in (sel_projects if sel_projects else projects):
    d = df_f[df_f["project"] == proj].sort_values("view", ascending=False)
    if d.empty:
        continue

    top = d.iloc[0]
    bottom = d.iloc[-1]

    st.markdown(f"### é¡¹ç›®ï¼š{proj}")
    left, right = st.columns(2)

    def render_card(col, row, tag):
        up = row.get("owner_name", "")
        base = df[df["owner_name"] == up] if up else df

        mean_v, std_v = base["view"].mean(), base["view"].std(ddof=0)
        mean_er, std_er = base["engagement_rate"].mean(), base["engagement_rate"].std(ddof=0)

        col.markdown(f"**{tag}ï¼š{row.get('title','')}**")
        col.caption(f"UPï¼š{up} ï½œ BVï¼š{row.get('bvid','')} ï½œ å‘å¸ƒï¼š{row.get('pubdate','')}")
        col.metric("æ’­æ”¾", f"{int(row.get('view',0)):,}", label_vs_baseline(float(row.get("view",0)), mean_v, std_v))
        col.metric(
            "äº’åŠ¨ç‡",
            f"{float(row.get('engagement_rate',0))*100:.2f}%",
            label_vs_baseline(float(row.get("engagement_rate",0)), mean_er, std_er),
        )
        col.write(
            f"- èµ/å¸/è—/è¯„ï¼š{int(row.get('like',0))}/{int(row.get('coin',0))}/{int(row.get('favorite',0))}/{int(row.get('reply',0))}"
        )
        col.write(f"- æ·±åº¦ä¿¡å·å æ¯”ï¼š{float(row.get('deep_signal_ratio',0))*100:.1f}%")

    render_card(left, top, "ğŸ”¥ æœ€é«˜æ’­æ”¾")
    render_card(right, bottom, "ğŸ§Š æœ€ä½æ’­æ”¾")

# =========================
# äº’åŠ¨ç‡åˆ†å¸ƒï¼ˆä¿æŒä¸å˜ï¼‰
# =========================
st.subheader("äº’åŠ¨ç‡åˆ†å¸ƒï¼ˆé¡¹ç›®/UPä¸»å¿«é€Ÿå®šä½å¼‚å¸¸ï¼‰")
fig = px.box(df_f, x="project", y="engagement_rate", points="all", hover_data=["title","owner_name","view"])
st.plotly_chart(fig, use_container_width=True)

# =========================
# è‡ªåŠ¨è§£è¯»ï¼ˆä¿æŒä¸å˜ï¼‰
# =========================
st.subheader("è‡ªåŠ¨è§£è¯»ï¼ˆå¯å¤åˆ¶è¿›å‘¨æŠ¥ï¼‰")
best = df_f.sort_values("view", ascending=False).iloc[0]
worst = df_f.sort_values("view", ascending=True).iloc[0]
insights = []
insights.append(f"1ï¼‰æœ¬æœŸæœ€é«˜æ’­æ”¾æ¥è‡ªã€Š{best['title']}ã€‹ï¼ˆ{int(best['view']):,} æ’­æ”¾ï¼‰ï¼Œäº’åŠ¨ç‡ {best['engagement_rate']*100:.2f}%ï¼Œæ·±åº¦ä¿¡å·å æ¯” {best['deep_signal_ratio']*100:.1f}%ã€‚")
insights.append(f"2ï¼‰æœ€ä½æ’­æ”¾ä¸ºã€Š{worst['title']}ã€‹ï¼ˆ{int(worst['view']):,} æ’­æ”¾ï¼‰ï¼Œäº’åŠ¨ç‡ {worst['engagement_rate']*100:.2f}%ã€‚å»ºè®®æ£€æŸ¥å°é¢/æ ‡é¢˜ä¿¡æ¯å¯†åº¦ä¸æŠ•æ”¾æ—¶æ®µï¼Œå¹¶åœ¨è¯„è®ºåŒºåšæ›´å¼ºçš„äº’åŠ¨å¼•å¯¼ã€‚")
if df_f["deep_signal_ratio"].mean() < 0.35:
    insights.append("3ï¼‰æ•´ä½“æ·±åº¦ä¿¡å·åä½ï¼ˆå¸+è—åœ¨äº’åŠ¨ä¸­çš„å æ¯”ä¸é«˜ï¼‰ï¼Œè¯´æ˜å†…å®¹æ›´å¤šæ˜¯â€œè·¯è¿‡å‹çƒ­åº¦â€ï¼Œå»ºè®®å¼ºåŒ–ï¼šä»·å€¼ç‚¹å‰ç½®ã€ç»“å°¾å¼•å¯¼æ”¶è—/æŠ•å¸ã€å¢åŠ ç³»åˆ—åŒ–æ‰¿è¯ºã€‚")
else:
    insights.append("3ï¼‰æ•´ä½“æ·±åº¦ä¿¡å·å¥åº·ï¼ˆå¸+è—å æ¯”é«˜ï¼‰ï¼Œè¯´æ˜å†…å®¹å…·å¤‡æ²‰æ·€å±æ€§ï¼Œå¯è€ƒè™‘å›´ç»•è¯¥æ–¹å‘åšç³»åˆ—åŒ–ä¸å›ºå®šæ ç›®èŠ‚å¥ã€‚")
st.write("\n".join(insights))
